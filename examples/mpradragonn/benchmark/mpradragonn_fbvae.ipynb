{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras import layers, initializers\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import json\n",
    "#import tensorflow_probability as tfp\n",
    "\n",
    "#tfd = tfp.distributions\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "def contain_tf_gpu_mem_usage() :\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)\n",
    "\n",
    "contain_tf_gpu_mem_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "\n",
    "#Stochastic Binarized Neuron helper functions (Tensorflow)\n",
    "#ST Estimator code adopted from https://r2rt.com/beyond-binary-ternary-and-one-hot-neurons.html\n",
    "#See Github https://github.com/spitis/\n",
    "\n",
    "def st_sampled_softmax(logits):\n",
    "    with ops.name_scope(\"STSampledSoftmax\") as namescope :\n",
    "        nt_probs = tf.nn.softmax(logits)\n",
    "        onehot_dim = logits.get_shape().as_list()[1]\n",
    "        sampled_onehot = tf.one_hot(tf.squeeze(tf.multinomial(logits, 1), 1), onehot_dim, 1.0, 0.0)\n",
    "        with tf.get_default_graph().gradient_override_map({'Ceil': 'Identity', 'Mul': 'STMul'}):\n",
    "            return tf.ceil(sampled_onehot * nt_probs)\n",
    "\n",
    "def st_hardmax_softmax(logits):\n",
    "    with ops.name_scope(\"STHardmaxSoftmax\") as namescope :\n",
    "        nt_probs = tf.nn.softmax(logits)\n",
    "        onehot_dim = logits.get_shape().as_list()[1]\n",
    "        sampled_onehot = tf.one_hot(tf.argmax(nt_probs, 1), onehot_dim, 1.0, 0.0)\n",
    "        with tf.get_default_graph().gradient_override_map({'Ceil': 'Identity', 'Mul': 'STMul'}):\n",
    "            return tf.ceil(sampled_onehot * nt_probs)\n",
    "\n",
    "@ops.RegisterGradient(\"STMul\")\n",
    "def st_mul(op, grad):\n",
    "    return [grad, grad]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import isolearn.io as isoio\n",
    "import isolearn.keras as isol\n",
    "\n",
    "encoder = isol.OneHotEncoder(seq_length=145)\n",
    "\n",
    "def initialize_sequence_templates(sequence_template, encoder=encoder) :\n",
    "\n",
    "    onehot_template = encoder(sequence_template).reshape((1, len(sequence_template), 4, 1))\n",
    "\n",
    "    for j in range(len(sequence_template)) :\n",
    "        if sequence_template[j] != 'N' :\n",
    "            nt_ix = np.argmax(onehot_template[0, j, :, 0])\n",
    "            onehot_template[0, j, :, :] = 0\n",
    "            onehot_template[0, j, nt_ix, :] = 1\n",
    "        else :\n",
    "            onehot_template[0, j, :, :] = 0\n",
    "\n",
    "    onehot_mask = np.zeros((1, len(sequence_template), 4, 1))\n",
    "    for j in range(len(sequence_template)) :\n",
    "        if sequence_template[j] == 'N' :\n",
    "            onehot_mask[0, j, :, :] = 1.0\n",
    "\n",
    "    return onehot_template, onehot_mask\n",
    "\n",
    "sequence_template = 'N' * 145\n",
    "\n",
    "template_mat, mask_mat = initialize_sequence_templates(sequence_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, ConvLSTM2D, GRU, BatchNormalization, LocallyConnected2D, Permute\n",
    "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from keras.layers.merge import _Merge\n",
    "import keras.losses\n",
    "\n",
    "def make_gen_resblock(n_channels=64, window_size=3, stride=1, dilation=1, group_ix=0, layer_ix=0) :\n",
    "\n",
    "    #Initialize res block layers\n",
    "    batch_norm_0 = BatchNormalization(name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_batch_norm_0')\n",
    "\n",
    "    relu_0 = Lambda(lambda x: K.relu(x))\n",
    "    \n",
    "    deconv_0 = Conv2DTranspose(n_channels, (1, window_size), strides=(1, stride), padding='same', activation='linear', kernel_initializer='glorot_uniform', name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_deconv_0')\n",
    "\n",
    "    batch_norm_1 = BatchNormalization(name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_batch_norm_1')\n",
    "\n",
    "    relu_1 = Lambda(lambda x: K.relu(x))\n",
    "\n",
    "    conv_1 = Conv2D(n_channels, (1, window_size), dilation_rate=(1, dilation), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_uniform', name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_conv_1')\n",
    "\n",
    "    skip_deconv_0 = Conv2DTranspose(n_channels, (1, 1), strides=(1, stride), padding='same', activation='linear', kernel_initializer='glorot_uniform', name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_skip_deconv_0')\n",
    "    \n",
    "    skip_1 = Lambda(lambda x: x[0] + x[1], name='policy_generator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_skip_1')\n",
    "\n",
    "    #Execute res block\n",
    "    def _resblock_func(input_tensor) :\n",
    "        \n",
    "        batch_norm_0_out = batch_norm_0(input_tensor)\n",
    "        relu_0_out = relu_0(batch_norm_0_out)\n",
    "        deconv_0_out = deconv_0(relu_0_out)\n",
    "\n",
    "        batch_norm_1_out = batch_norm_1(deconv_0_out)\n",
    "        relu_1_out = relu_1(batch_norm_1_out)\n",
    "        conv_1_out = conv_1(relu_1_out)\n",
    "        \n",
    "        skip_deconv_0_out = skip_deconv_0(input_tensor)\n",
    "\n",
    "        skip_1_out = skip_1([conv_1_out, skip_deconv_0_out])\n",
    "        \n",
    "        return skip_1_out\n",
    "\n",
    "    return _resblock_func\n",
    "\n",
    "#Decoder Model definition\n",
    "def load_decoder_resnet(seq_length=145, latent_size=100) :\n",
    "\n",
    "    #Generator network parameters\n",
    "    window_size = 3\n",
    "    \n",
    "    strides = [2, 2, 2, 2, 2, 1]\n",
    "    dilations = [1, 1, 1, 1, 1, 1]\n",
    "    channels = [256, 128, 96, 64, 32, 32]#[384, 256, 128, 64, 32]\n",
    "    initial_length = 5\n",
    "    n_resblocks = len(strides)\n",
    "\n",
    "    #Policy network definition\n",
    "    policy_dense_0 = Dense(initial_length * channels[0], activation='linear', kernel_initializer='glorot_uniform', name='policy_generator_dense_0')\n",
    "    policy_dense_0_reshape = Reshape((1, initial_length, channels[0]))\n",
    "    \n",
    "    curr_length = initial_length\n",
    "    \n",
    "    resblocks = []\n",
    "    for layer_ix in range(n_resblocks) :\n",
    "        resblocks.append(make_gen_resblock(n_channels=channels[layer_ix], window_size=window_size, stride=strides[layer_ix], dilation=dilations[layer_ix], group_ix=0, layer_ix=layer_ix))\n",
    "    \n",
    "    final_conv = Conv2D(4, (1, 1), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_uniform', name='policy_generator_final_conv')\n",
    "    \n",
    "    final_slice = Lambda(lambda x, seq_length=seq_length: x[:, :, :seq_length, :])\n",
    "    \n",
    "    def _generator_func(seed_input) :\n",
    "        \n",
    "        policy_dense_0_out = policy_dense_0_reshape(policy_dense_0(seed_input))\n",
    "        \n",
    "        #Connect group of res blocks\n",
    "        output_tensor = policy_dense_0_out\n",
    "\n",
    "        #Res block group 0\n",
    "        for layer_ix in range(n_resblocks) :\n",
    "            output_tensor = resblocks[layer_ix](output_tensor)\n",
    "\n",
    "        #Final conv out\n",
    "        final_conv_out = final_conv(output_tensor)#final_conv(final_relu_out)\n",
    "        \n",
    "        return final_slice(final_conv_out)\n",
    "\n",
    "    return _generator_func\n",
    "\n",
    "\n",
    "def make_disc_resblock(n_channels=64, window_size=8, dilation_rate=1, group_ix=0, layer_ix=0) :\n",
    "\n",
    "    #Initialize res block layers\n",
    "    batch_norm_0 = BatchNormalization(name='policy_discriminator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_batch_norm_0')\n",
    "\n",
    "    relu_0 = Lambda(lambda x: K.relu(x, alpha=0.0))\n",
    "\n",
    "    conv_0 = Conv2D(n_channels, (1, window_size), dilation_rate=dilation_rate, strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_discriminator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_conv_0')\n",
    "\n",
    "    batch_norm_1 = BatchNormalization(name='policy_discriminator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_batch_norm_1')\n",
    "\n",
    "    relu_1 = Lambda(lambda x: K.relu(x, alpha=0.0))\n",
    "\n",
    "    conv_1 = Conv2D(n_channels, (1, window_size), dilation_rate=dilation_rate, strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_discriminator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_conv_1')\n",
    "\n",
    "    skip_1 = Lambda(lambda x: x[0] + x[1], name='policy_discriminator_resblock_' + str(group_ix) + '_' + str(layer_ix) + '_skip_1')\n",
    "\n",
    "    #Execute res block\n",
    "    def _resblock_func(input_tensor) :\n",
    "        batch_norm_0_out = batch_norm_0(input_tensor)\n",
    "        relu_0_out = relu_0(batch_norm_0_out)\n",
    "        conv_0_out = conv_0(relu_0_out)\n",
    "\n",
    "        batch_norm_1_out = batch_norm_1(conv_0_out)\n",
    "        relu_1_out = relu_1(batch_norm_1_out)\n",
    "        conv_1_out = conv_1(relu_1_out)\n",
    "\n",
    "        skip_1_out = skip_1([conv_1_out, input_tensor])\n",
    "        \n",
    "        return skip_1_out\n",
    "\n",
    "    return _resblock_func\n",
    "\n",
    "#Encoder Model definition\n",
    "def load_encoder_network_4_resblocks(batch_size, seq_length=205, latent_size=100, drop_rate=0.25) :\n",
    "\n",
    "    #Discriminator network parameters\n",
    "    n_resblocks = 4\n",
    "    n_channels = 32\n",
    "\n",
    "    #Discriminator network definition\n",
    "    policy_conv_0 = Conv2D(n_channels, (1, 1), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_discriminator_conv_0')\n",
    "    \n",
    "    skip_conv_0 = Conv2D(n_channels, (1, 1), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_discriminator_skip_conv_0')\n",
    "    \n",
    "    resblocks = []\n",
    "    for layer_ix in range(n_resblocks) :\n",
    "        resblocks.append(make_disc_resblock(n_channels=n_channels, window_size=8, dilation_rate=1, group_ix=0, layer_ix=layer_ix))\n",
    "    \n",
    "    last_block_conv = Conv2D(n_channels, (1, 1), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_discriminator_last_block_conv')\n",
    "    \n",
    "    skip_add = Lambda(lambda x: x[0] + x[1], name='policy_discriminator_skip_add')\n",
    "    \n",
    "    final_flatten = Flatten()\n",
    "    \n",
    "    z_mean = Dense(latent_size, name='policy_discriminator_z_mean')\n",
    "    z_log_var = Dense(latent_size, name='policy_discriminator_z_log_var')\n",
    "    \n",
    "    def _encoder_func(sequence_input) :\n",
    "        policy_conv_0_out = policy_conv_0(sequence_input)\n",
    "\n",
    "        #Connect group of res blocks\n",
    "        output_tensor = policy_conv_0_out\n",
    "\n",
    "        #Res block group 0\n",
    "        skip_conv_0_out = skip_conv_0(output_tensor)\n",
    "\n",
    "        for layer_ix in range(n_resblocks) :\n",
    "            output_tensor = resblocks[layer_ix](output_tensor)\n",
    "        \n",
    "        #Last res block extr conv\n",
    "        last_block_conv_out = last_block_conv(output_tensor)\n",
    "\n",
    "        skip_add_out = skip_add([last_block_conv_out, skip_conv_0_out])\n",
    "\n",
    "        #Final dense out\n",
    "        final_dense_out = final_flatten(skip_add_out)\n",
    "        \n",
    "        #Z mean and log variance\n",
    "        z_mean_out = z_mean(final_dense_out)\n",
    "        z_log_var_out = z_log_var(final_dense_out)\n",
    "\n",
    "        return z_mean_out, z_log_var_out\n",
    "\n",
    "    return _encoder_func\n",
    "\n",
    "#Encoder Model definition\n",
    "def load_encoder_network_8_resblocks(batch_size, seq_length=128, drop_rate=0.25) :\n",
    "\n",
    "    #Discriminator network parameters\n",
    "    n_resblocks = 4\n",
    "    n_channels = 32\n",
    "    latent_size = 100\n",
    "\n",
    "    #Discriminator network definition\n",
    "    policy_conv_0 = Conv2D(n_channels, (1, 1), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_discriminator_conv_0')\n",
    "    \n",
    "    #Res block group 0\n",
    "    skip_conv_0 = Conv2D(n_channels, (1, 1), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_discriminator_skip_conv_0')\n",
    "    \n",
    "    resblocks_0 = []\n",
    "    for layer_ix in range(n_resblocks) :\n",
    "        resblocks_0.append(make_disc_resblock(n_channels=n_channels, window_size=8, dilation_rate=1, group_ix=0, layer_ix=layer_ix))\n",
    "    \n",
    "    #Res block group 1\n",
    "    skip_conv_1 = Conv2D(n_channels, (1, 1), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_discriminator_skip_conv_1')\n",
    "    \n",
    "    resblocks_1 = []\n",
    "    for layer_ix in range(n_resblocks) :\n",
    "        resblocks_1.append(make_disc_resblock(n_channels=n_channels, window_size=8, dilation_rate=4, group_ix=1, layer_ix=layer_ix))\n",
    "    \n",
    "    last_block_conv = Conv2D(n_channels, (1, 1), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_discriminator_last_block_conv')\n",
    "    \n",
    "    skip_add = Lambda(lambda x: x[0] + x[1] + x[2], name='policy_discriminator_skip_add')\n",
    "    \n",
    "    final_flatten = Flatten()\n",
    "    \n",
    "    z_mean = Dense(latent_size, name='policy_discriminator_z_mean')\n",
    "    z_log_var = Dense(latent_size, name='policy_discriminator_z_log_var')\n",
    "    \n",
    "    def _encoder_func(sequence_input) :\n",
    "        policy_conv_0_out = policy_conv_0(sequence_input)\n",
    "\n",
    "        #Connect group of res blocks\n",
    "        output_tensor = policy_conv_0_out\n",
    "\n",
    "        #Res block group 0\n",
    "        skip_conv_0_out = skip_conv_0(output_tensor)\n",
    "\n",
    "        for layer_ix in range(n_resblocks) :\n",
    "            output_tensor = resblocks_0[layer_ix](output_tensor)\n",
    "        \n",
    "        #Res block group 0\n",
    "        skip_conv_1_out = skip_conv_1(output_tensor)\n",
    "\n",
    "        for layer_ix in range(n_resblocks) :\n",
    "            output_tensor = resblocks_1[layer_ix](output_tensor)\n",
    "        \n",
    "        #Last res block extr conv\n",
    "        last_block_conv_out = last_block_conv(output_tensor)\n",
    "\n",
    "        skip_add_out = skip_add([last_block_conv_out, skip_conv_0_out, skip_conv_1_out])\n",
    "\n",
    "        #Final dense out\n",
    "        final_dense_out = final_flatten(skip_add_out)\n",
    "        \n",
    "        #Z mean and log variance\n",
    "        z_mean_out = z_mean(final_dense_out)\n",
    "        z_log_var_out = z_log_var(final_dense_out)\n",
    "\n",
    "        return z_mean_out, z_log_var_out\n",
    "\n",
    "    return _encoder_func\n",
    "\n",
    "#PWM Masking and Sampling helper functions\n",
    "\n",
    "def mask_pwm(inputs) :\n",
    "    pwm, onehot_template, onehot_mask = inputs\n",
    "\n",
    "    return pwm * onehot_mask + onehot_template\n",
    "\n",
    "def sample_pwm_only(pwm_logits) :\n",
    "    n_sequences = K.shape(pwm_logits)[0]\n",
    "    seq_length = K.shape(pwm_logits)[2]\n",
    "\n",
    "    flat_pwm = K.reshape(pwm_logits, (n_sequences * seq_length, 4))\n",
    "    sampled_pwm = st_sampled_softmax(flat_pwm)\n",
    "\n",
    "    return K.reshape(sampled_pwm, (n_sequences, 1, seq_length, 4))\n",
    "\n",
    "def sample_pwm(pwm_logits) :\n",
    "    n_sequences = K.shape(pwm_logits)[0]\n",
    "    seq_length = K.shape(pwm_logits)[2]\n",
    "\n",
    "    flat_pwm = K.reshape(pwm_logits, (n_sequences * seq_length, 4))\n",
    "    sampled_pwm = sampled_pwm = K.switch(K.learning_phase(), st_sampled_softmax(flat_pwm), st_hardmax_softmax(flat_pwm))\n",
    "\n",
    "    return K.reshape(sampled_pwm, (n_sequences, 1, seq_length, 4))\n",
    "\n",
    "def max_pwm(pwm_logits) :\n",
    "    n_sequences = K.shape(pwm_logits)[0]\n",
    "    seq_length = K.shape(pwm_logits)[2]\n",
    "\n",
    "    flat_pwm = K.reshape(pwm_logits, (n_sequences * seq_length, 4))\n",
    "    sampled_pwm = sampled_pwm = st_hardmax_softmax(flat_pwm)\n",
    "\n",
    "    return K.reshape(sampled_pwm, (n_sequences, 1, seq_length, 4))\n",
    "\n",
    "\n",
    "#Generator helper functions\n",
    "def initialize_sequence_templates_model(generator, sequence_templates) :\n",
    "\n",
    "    embedding_templates = []\n",
    "    embedding_masks = []\n",
    "\n",
    "    for k in range(len(sequence_templates)) :\n",
    "        sequence_template = sequence_templates[k]\n",
    "        onehot_template = isol.OneHotEncoder(seq_length=len(sequence_template))(sequence_template).reshape((1, len(sequence_template), 4))\n",
    "\n",
    "        for j in range(len(sequence_template)) :\n",
    "            if sequence_template[j] not in ['N', 'X'] :\n",
    "                nt_ix = np.argmax(onehot_template[0, j, :])\n",
    "                onehot_template[:, j, :] = -4.0\n",
    "                onehot_template[:, j, nt_ix] = 10.0\n",
    "            elif sequence_template[j] == 'X' :\n",
    "                onehot_template[:, j, :] = -1.0\n",
    "\n",
    "        onehot_mask = np.zeros((1, len(sequence_template), 4))\n",
    "        for j in range(len(sequence_template)) :\n",
    "            if sequence_template[j] == 'N' :\n",
    "                onehot_mask[:, j, :] = 1.0\n",
    "\n",
    "        embedding_templates.append(onehot_template.reshape(1, -1))\n",
    "        embedding_masks.append(onehot_mask.reshape(1, -1))\n",
    "\n",
    "    embedding_templates = np.concatenate(embedding_templates, axis=0)\n",
    "    embedding_masks = np.concatenate(embedding_masks, axis=0)\n",
    "\n",
    "    generator.get_layer('template_dense').set_weights([embedding_templates])\n",
    "    generator.get_layer('template_dense').trainable = False\n",
    "\n",
    "    generator.get_layer('mask_dense').set_weights([embedding_masks])\n",
    "    generator.get_layer('mask_dense').trainable = False\n",
    "\n",
    "\n",
    "#Generator construction function\n",
    "def build_sampler(batch_size, seq_length, n_classes=1, n_samples=None, validation_sample_mode='max') :\n",
    "\n",
    "    use_samples = True\n",
    "    if n_samples is None :\n",
    "        use_samples = False\n",
    "        n_samples = 1\n",
    "\n",
    "    \n",
    "    #Initialize Reshape layer\n",
    "    reshape_layer = Reshape((1, seq_length, 4))\n",
    "\n",
    "    #Initialize template and mask matrices\n",
    "    onehot_template_dense = Embedding(n_classes, seq_length * 4, embeddings_initializer='zeros', name='template_dense')\n",
    "    onehot_mask_dense = Embedding(n_classes, seq_length * 4, embeddings_initializer='ones', name='mask_dense')\n",
    "\n",
    "    #Initialize Templating and Masking Lambda layer\n",
    "    masking_layer = Lambda(mask_pwm, output_shape = (1, seq_length, 4), name='masking_layer')\n",
    "    \n",
    "    #Initialize PWM normalization layer\n",
    "    pwm_layer = Softmax(axis=-1, name='pwm')\n",
    "    \n",
    "    #Initialize sampling layers\n",
    "    sample_func = sample_pwm\n",
    "    if validation_sample_mode == 'sample' :\n",
    "        sample_func = sample_pwm_only\n",
    "    \n",
    "    upsampling_layer = Lambda(lambda x: K.tile(x, [n_samples, 1, 1, 1]), name='upsampling_layer')\n",
    "    sampling_layer = Lambda(sample_func, name='pwm_sampler')\n",
    "    permute_layer = Lambda(lambda x: K.permute_dimensions(K.reshape(x, (n_samples, batch_size, 1, seq_length, 4)), (1, 0, 2, 3, 4)), name='permute_layer')\n",
    "    \n",
    "    \n",
    "    def _sampler_func(class_input, raw_logits) :\n",
    "        \n",
    "        #Get Template and Mask\n",
    "        onehot_template = reshape_layer(onehot_template_dense(class_input))\n",
    "        onehot_mask = reshape_layer(onehot_mask_dense(class_input))\n",
    "        \n",
    "        #Add Template and Multiply Mask\n",
    "        pwm_logits = masking_layer([raw_logits, onehot_template, onehot_mask])\n",
    "        \n",
    "        #Compute PWM (Nucleotide-wise Softmax)\n",
    "        pwm = pwm_layer(pwm_logits)\n",
    "        \n",
    "        sampled_pwm = None\n",
    "        \n",
    "        #Optionally tile each PWM to sample from and create sample axis\n",
    "        if use_samples :\n",
    "            pwm_logits_upsampled = upsampling_layer(pwm_logits)\n",
    "            sampled_pwm = sampling_layer(pwm_logits_upsampled)\n",
    "            sampled_pwm = permute_layer(sampled_pwm)\n",
    "        else :\n",
    "            sampled_pwm = sampling_layer(pwm_logits)\n",
    "        \n",
    "        \n",
    "        return pwm_logits, pwm, sampled_pwm\n",
    "    \n",
    "    return _sampler_func\n",
    "\n",
    "\n",
    "def get_pwm_cross_entropy(pwm_start, pwm_end) :\n",
    "\n",
    "    def _pwm_cross_entropy(inputs) :\n",
    "        pwm_true, pwm_pred = inputs\n",
    "        \n",
    "        pwm_pred = K.clip(pwm_pred, K.epsilon(), 1. - K.epsilon())\n",
    "\n",
    "        ce = - K.sum(pwm_true[:, 0, pwm_start:pwm_end, :] * K.log(pwm_pred[:, 0, pwm_start:pwm_end, :]), axis=-1)\n",
    "        \n",
    "        return K.expand_dims(K.mean(ce, axis=-1), axis=-1)\n",
    "    \n",
    "    return _pwm_cross_entropy\n",
    "\n",
    "def min_pred(y_true, y_pred) :\n",
    "    return y_pred\n",
    "\n",
    "def get_weighted_loss(loss_coeff=1.) :\n",
    "    \n",
    "    def _min_pred(y_true, y_pred) :\n",
    "        return loss_coeff * y_pred\n",
    "    \n",
    "    return _min_pred\n",
    "\n",
    "def get_z_sample(z_inputs):\n",
    "    \n",
    "    z_mean, z_log_var = z_inputs\n",
    "    \n",
    "    batch_size = K.shape(z_mean)[0]\n",
    "    latent_dim = K.int_shape(z_mean)[1]\n",
    "    \n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim))\n",
    "    \n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def get_z_kl_loss(anneal_coeff) :\n",
    "    \n",
    "    def _z_kl_loss(inputs, anneal_coeff=anneal_coeff) :\n",
    "        z_mean, z_log_var = inputs\n",
    "        \n",
    "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "        kl_loss = K.mean(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        \n",
    "        return anneal_coeff * K.expand_dims(kl_loss, axis=-1)\n",
    "    \n",
    "    return _z_kl_loss\n",
    "\n",
    "def build_vae(model_path) :\n",
    "    \n",
    "    #Simple Library\n",
    "    sequence_templates = [\n",
    "        'N' * 145\n",
    "    ]\n",
    "\n",
    "    #Initialize Encoder and Decoder networks\n",
    "    batch_size = 32\n",
    "    seq_length = 145\n",
    "    n_samples = None\n",
    "    latent_size = 100\n",
    "\n",
    "    #Load Encoder\n",
    "    encoder = load_encoder_network_4_resblocks(batch_size, seq_length=seq_length, latent_size=latent_size, drop_rate=0.)\n",
    "\n",
    "    #Load Decoder\n",
    "    decoder = load_decoder_resnet(seq_length=seq_length, latent_size=latent_size)\n",
    "\n",
    "    #Load Sampler\n",
    "    sampler = build_sampler(batch_size, seq_length, n_classes=1, n_samples=n_samples, validation_sample_mode='sample')\n",
    "\n",
    "    #Build Encoder Model\n",
    "    encoder_input = Input(shape=(1, seq_length, 4), name='encoder_input')\n",
    "\n",
    "    z_mean, z_log_var = encoder(encoder_input)\n",
    "\n",
    "    z_sampling_layer = Lambda(get_z_sample, output_shape=(latent_size,), name='z_sampler')\n",
    "    z = z_sampling_layer([z_mean, z_log_var])\n",
    "\n",
    "    # instantiate encoder model\n",
    "    encoder_model = Model(encoder_input, [z_mean, z_log_var, z])\n",
    "    #encoder_model.compile(\n",
    "    #    optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999),\n",
    "    #    loss=min_pred\n",
    "    #)\n",
    "\n",
    "    #Build Decoder Model\n",
    "    decoder_class = Input(shape=(1,), name='decoder_class')\n",
    "    decoder_input = Input(shape=(latent_size,), name='decoder_input')\n",
    "\n",
    "    pwm_logits, pwm, sampled_pwm = sampler(decoder_class, decoder(decoder_input))\n",
    "\n",
    "    decoder_model = Model([decoder_class, decoder_input], [pwm_logits, pwm, sampled_pwm])\n",
    "\n",
    "    #Initialize Sequence Templates and Masks\n",
    "    initialize_sequence_templates_model(decoder_model, sequence_templates)\n",
    "\n",
    "    #decoder_model.compile(\n",
    "    #    optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999),\n",
    "    #    loss=min_pred\n",
    "    #)\n",
    "    \n",
    "    vae_decoder_class = Input(shape=(1,), name='vae_decoder_class')\n",
    "    vae_encoder_input = Input(shape=(1, seq_length, 4), name='vae_encoder_input')\n",
    "\n",
    "    encoded_z_mean, encoded_z_log_var = encoder(vae_encoder_input)\n",
    "    encoded_z = z_sampling_layer([encoded_z_mean, encoded_z_log_var])\n",
    "    decoded_logits, decoded_pwm, decoded_sample = sampler(vae_decoder_class, decoder(encoded_z))\n",
    "\n",
    "    reconstruction_loss = Lambda(get_pwm_cross_entropy(pwm_start=0, pwm_end=145), name='reconstruction')([vae_encoder_input, decoded_pwm])\n",
    "\n",
    "    anneal_coeff = K.variable(1.0)\n",
    "\n",
    "    kl_loss = Lambda(get_z_kl_loss(anneal_coeff), name='kl')([encoded_z_mean, encoded_z_log_var])\n",
    "\n",
    "    vae_model = Model(\n",
    "        [vae_decoder_class, vae_encoder_input],\n",
    "        [reconstruction_loss, kl_loss]\n",
    "    )\n",
    "\n",
    "    #Initialize Sequence Templates and Masks\n",
    "    initialize_sequence_templates_model(vae_model, sequence_templates)\n",
    "\n",
    "    vae_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=0.0001, beta_1=0.5, beta_2=0.9),\n",
    "        loss={\n",
    "            'reconstruction' : get_weighted_loss(loss_coeff=1.),\n",
    "            'kl' : get_weighted_loss(loss_coeff=0.5)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return vae_model, encoder_model, decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _templated_predict(oracle, x, batch_size=32, template=template_mat, mask=mask_mat) :\n",
    "    \n",
    "    onehots = np.transpose(x, (0, 2, 3, 1)) * mask + template\n",
    "\n",
    "    #Predict fitness\n",
    "    score_pred = oracle.predict(x=[onehots[..., 0]], batch_size=batch_size)[:, 5]\n",
    "    \n",
    "    return score_pred\n",
    "\n",
    "def fb_opt(X_train, vae_model_path, oracle, vae_0_encoder, vae_0_decoder, weights_type='fbvae',\n",
    "        LD=100, iters=20, samples=500, \n",
    "        quantile=0.8, verbose=False, train_gt_evals=None,\n",
    "        it_epochs=10, enc1_units=50, store_every=1, sequence_template=sequence_template):\n",
    "    \n",
    "    assert weights_type in ['fbvae']\n",
    "    L = X_train.shape[1]\n",
    "    \n",
    "    vae_model, vae_encoder, vae_decoder = build_vae(vae_model_path)\n",
    "    \n",
    "    def get_samples(Xt_p):\n",
    "        Xt_sampled = np.zeros_like(Xt_p)\n",
    "        for i in range(Xt_p.shape[0]):\n",
    "            for j in range(Xt_p.shape[2]):\n",
    "                p = Xt_p[i, 0, j, :]\n",
    "                k = np.random.choice(range(len(p)), p=p)\n",
    "                Xt_sampled[i, 0, j, k] = 1.\n",
    "        return Xt_sampled\n",
    "\n",
    "    generated_sequences = []\n",
    "    fb_thresh = -np.inf\n",
    "    n_top = 0\n",
    "    \n",
    "    for t in range(iters):\n",
    "        ### Take Samples and evaluate ground truth and oracle ##\n",
    "        zt = np.random.randn(samples, LD)\n",
    "        zt_dummy = np.zeros((samples, 1))\n",
    "        if t > 0:\n",
    "            Xt_sample_p = vae_decoder.predict([zt_dummy, zt])[1]\n",
    "            Xt_sample = get_samples(Xt_sample_p)\n",
    "            yt_sample = _templated_predict(oracle, Xt_sample, batch_size=32)\n",
    "        else:\n",
    "            Xt_sample_p = vae_0_decoder.predict([zt_dummy, zt])[1]\n",
    "            Xt_sample = get_samples(Xt_sample_p)\n",
    "            yt_sample = _templated_predict(oracle, Xt_sample, batch_size=32)\n",
    "            Xt = X_train\n",
    "            yt = _templated_predict(oracle, Xt, batch_size=32)\n",
    "            fb_thresh = np.percentile(yt, quantile*100)\n",
    "        \n",
    "        ### Calculate threshold ###\n",
    "        if t > 0:\n",
    "            threshold_idx = np.where(yt_sample >= fb_thresh)[0]\n",
    "            n_top = len(threshold_idx)\n",
    "            sample_arrs = [Xt_sample, yt_sample]\n",
    "            full_arrs = [Xt, yt]\n",
    "            \n",
    "            for l in range(len(full_arrs)):\n",
    "                sample_arr = sample_arrs[l]\n",
    "                full_arr = full_arrs[l]\n",
    "                sample_top = sample_arr[threshold_idx]\n",
    "                full_arr = np.concatenate([sample_top, full_arr])\n",
    "                full_arr = np.delete(full_arr, range(full_arr.shape[0]-n_top, full_arr.shape[0]), axis=0)\n",
    "                full_arrs[l] = full_arr\n",
    "            Xt, yt = full_arrs\n",
    "        \n",
    "        if t % store_every == 0 :\n",
    "            Xt_sample_seqs = []\n",
    "            nt_map_inv = {0:'A', 1:'C', 2:'G', 3:'T'}\n",
    "            \n",
    "            for i in range(Xt_sample.shape[0]) :\n",
    "                xt_seq = ''\n",
    "                for j in range(Xt_sample.shape[2]) :\n",
    "                    argmax_j = np.argmax(Xt_sample[i, 0, j, :])\n",
    "                    xt_seq += nt_map_inv[argmax_j]\n",
    "                \n",
    "                Xt_sample_seqs.append(xt_seq)\n",
    "            \n",
    "            generated_sequences.append(Xt_sample_seqs)\n",
    "        \n",
    "        if verbose:\n",
    "            print(weights_type.upper(), t, fb_thresh, np.median(yt_sample), n_top)\n",
    "        \n",
    "        ### Train model ###\n",
    "        if t == 0:\n",
    "            vae_encoder.load_weights(vae_model_path + \"_encoder.h5\", by_name=True)\n",
    "            vae_decoder.load_weights(vae_model_path + \"_decoder.h5\", by_name=True)\n",
    "        else:\n",
    "            dummy_train = np.zeros((Xt.shape[0], 1))\n",
    "            \n",
    "            # train the autoencoder\n",
    "            _ = vae_model.fit(\n",
    "                [dummy_train, Xt],\n",
    "                [dummy_train, dummy_train],\n",
    "                shuffle=False,\n",
    "                epochs=1,\n",
    "                batch_size=32,\n",
    "                verbose=1\n",
    "            )\n",
    "    \n",
    "    return generated_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(seqs) = 6000 (loaded)\n",
      "(5000, 1, 145, 4)\n",
      "(1000, 1, 145, 4)\n"
     ]
    }
   ],
   "source": [
    "#Load cached dataframe\n",
    "\n",
    "n_train = 5000\n",
    "n_test = 1000\n",
    "\n",
    "n_seqs = n_train + n_test\n",
    "\n",
    "seqs = []\n",
    "with open('../vae/mpradragonn_seqs_strong.txt', 'rt') as f :\n",
    "    for line_raw in f :\n",
    "        line = line_raw.strip()\n",
    "        seqs.append(line.split(\"\\t\")[0])\n",
    "        \n",
    "        if len(seqs) >= n_seqs :\n",
    "            break\n",
    "\n",
    "print(\"len(seqs) = \" + str(len(seqs)) + \" (loaded)\")\n",
    "\n",
    "short_encoder = isol.OneHotEncoder(145)\n",
    "\n",
    "x_train = np.concatenate([np.expand_dims(np.expand_dims(short_encoder(seq), axis=0), axis=0) for seq in seqs[:n_train]], axis=0)\n",
    "x_test = np.concatenate([np.expand_dims(np.expand_dims(short_encoder(seq), axis=0), axis=0) for seq in seqs[n_train:]], axis=0)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#Load predictor\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, ConvLSTM2D, GRU, BatchNormalization, LocallyConnected2D, Permute\n",
    "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import keras.losses\n",
    "\n",
    "def _load_mpradragonn_func(model_path) :\n",
    "\n",
    "    saved_model = Sequential()\n",
    "\n",
    "    # sublayer 1\n",
    "    saved_model.add(Conv1D(48, 3, padding='same', activation='relu', input_shape=(145, 4), name='dragonn_conv1d_1_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_1_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_1_copy'))\n",
    "\n",
    "    saved_model.add(Conv1D(64, 3, padding='same', activation='relu', name='dragonn_conv1d_2_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_2_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_2_copy'))\n",
    "\n",
    "    saved_model.add(Conv1D(100, 3, padding='same', activation='relu', name='dragonn_conv1d_3_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_3_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_3_copy'))\n",
    "\n",
    "    saved_model.add(Conv1D(150, 7, padding='same', activation='relu', name='dragonn_conv1d_4_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_4_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_4_copy'))\n",
    "\n",
    "    saved_model.add(Conv1D(300, 7, padding='same', activation='relu', name='dragonn_conv1d_5_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_5_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_5_copy'))\n",
    "\n",
    "    saved_model.add(MaxPooling1D(3))\n",
    "\n",
    "    # sublayer 2\n",
    "    saved_model.add(Conv1D(200, 7, padding='same', activation='relu', name='dragonn_conv1d_6_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_6_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_6_copy'))\n",
    "\n",
    "    saved_model.add(Conv1D(200, 3, padding='same', activation='relu', name='dragonn_conv1d_7_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_7_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_7_copy'))\n",
    "\n",
    "    saved_model.add(Conv1D(200, 3, padding='same', activation='relu', name='dragonn_conv1d_8_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_8_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_8_copy'))\n",
    "\n",
    "    saved_model.add(MaxPooling1D(4))\n",
    "\n",
    "    # sublayer 3\n",
    "    saved_model.add(Conv1D(200, 7, padding='same', activation='relu', name='dragonn_conv1d_9_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_9_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_9_copy'))\n",
    "\n",
    "    saved_model.add(MaxPooling1D(4))\n",
    "\n",
    "    saved_model.add(Flatten())\n",
    "    saved_model.add(Dense(100, activation='relu', name='dragonn_dense_1_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_10_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_10_copy'))\n",
    "    saved_model.add(Dense(12, activation='linear', name='dragonn_dense_2_copy'))\n",
    "\n",
    "    saved_model.compile(\n",
    "        loss= \"mean_squared_error\",\n",
    "        optimizer=keras.optimizers.SGD(lr=0.1)\n",
    "    )\n",
    "\n",
    "    saved_model.load_weights(model_path)\n",
    "    \n",
    "    return saved_model\n",
    "\n",
    "predictor_path = '../pretrained_deep_factorized_model.hdf5'\n",
    "\n",
    "oracle = _load_mpradragonn_func(predictor_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-7011d93d8e2f>:11: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n"
     ]
    }
   ],
   "source": [
    "#Load models\n",
    "\n",
    "vae_path = \"../vae/saved_models/vae_mpradragonn_max_activity_strong_len_145_latent_100_epochs_50_kl_factor_05_annealed\"\n",
    "\n",
    "vae_0_encoder = load_model(vae_path + '_encoder.h5', custom_objects={'st_sampled_softmax':st_sampled_softmax, 'st_hardmax_softmax':st_hardmax_softmax, 'min_pred':lambda y_true,y_pred:y_pred})\n",
    "vae_0_decoder = load_model(vae_path + '_decoder.h5', custom_objects={'st_sampled_softmax':st_sampled_softmax, 'st_hardmax_softmax':st_hardmax_softmax, 'min_pred':lambda y_true,y_pred:y_pred})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "vae_prefix_str = \"_epochs_50_kl_factor_05\"\n",
    "\n",
    "n_epochs = 150\n",
    "n_samples = 1000\n",
    "quantile = 0.8\n",
    "\n",
    "generated_sequences = fb_opt(x_train, vae_path, oracle, vae_0_encoder, vae_0_decoder,\n",
    "        LD=100, iters=n_epochs, samples=n_samples, \n",
    "        quantile=quantile, verbose=True, store_every=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed_suffix = \"\"\n",
    "\n",
    "experiment_name = \"mpradragonn_fb_vae\" + vae_prefix_str + \"_iters_\" + str(n_epochs) + \"_samples_\" + str(n_samples) + \"_q_\" + str(quantile).replace(\".\", \"\") + seed_suffix\n",
    "\n",
    "if not os.path.isdir('fbvae/' + experiment_name):\n",
    "    os.makedirs('fbvae/' + experiment_name)\n",
    "\n",
    "for epoch_i in range(n_epochs) :\n",
    "    with open('fbvae/' + experiment_name + \"/\" + \"iter_\" + str(epoch_i) + '.txt', 'wt') as f :\n",
    "        for seq in generated_sequences[epoch_i] :\n",
    "            f.write(seq + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
