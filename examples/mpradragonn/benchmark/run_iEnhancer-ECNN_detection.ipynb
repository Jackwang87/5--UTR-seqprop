{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: PhD. Nguyen Hong Quang\n",
    "# School of Information and Communication Technology\n",
    "# Hanoi University of Science and Technology\n",
    "# Email: quangnh@soict.hust.edu.vn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# HYPER-PARAMETERS ############\n",
    "FILE_MODEL_TMP = \"model_tmp.pkl\"\n",
    "\n",
    "MY_RANDOM_STATE = 5 \n",
    "torch.manual_seed(MY_RANDOM_STATE)\n",
    "\n",
    "SAMPLE_LENGTH = 200\n",
    "AVGPOOL1D_KERNEL_SIZE = 4\n",
    "CONV1D_KERNEL_SIZE = 3\n",
    "CONV1D_FEATURE_SIZE_BLOCK1 = 32\n",
    "CONV1D_FEATURE_SIZE_BLOCK2 = 64\n",
    "CONV1D_FEATURE_SIZE_BLOCK3 = 128\n",
    "\n",
    "FULLY_CONNECTED_LAYER_SIZE = 256\n",
    "\n",
    "MODEL_DIR = 'iEnhancer-ECNN/Train/model_layer1_seed' + str(MY_RANDOM_STATE)\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "###########################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {'A': 0,\n",
    "        'C': 1, \n",
    "        'G': 2,\n",
    "        'T':3,\n",
    "        'a':0,\n",
    "        'c':1,\n",
    "        'g':2,\n",
    "        't':3}\n",
    "\n",
    "# data = one_hot(1,3) ==> [0. 1. 0.]        \n",
    "def one_hot(index, dimension):\n",
    "    data = np.zeros((dimension))\n",
    "    data[index] = 1\n",
    "    return data\n",
    "\n",
    "#data = one_hot(1,3)\n",
    "#print(data)\n",
    "\n",
    "\n",
    "def load_text_file(file_text):\n",
    "    with open(file_text) as f:\n",
    "        lines = f.readlines()\n",
    "        my_data = [line.strip().upper() for line in lines[1::2]]\n",
    "        return my_data\n",
    "\n",
    "class EnhancerDataset(Dataset):\n",
    "    # X: list of Enhancer sequences (200 characters for each sequence)\n",
    "    # Y: list label [0, 1]; 0: negative, 1: positive\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.Y[index]\n",
    "        sample = self.X[index]\n",
    "        \n",
    "        values = np.zeros((4, SAMPLE_LENGTH))\n",
    "        for i in range(SAMPLE_LENGTH):\n",
    "            char_idx = my_dict[sample[i]]\n",
    "            values[char_idx, i] = 1 \n",
    "        \n",
    "        values_one_mer = self.extract_1_mer(sample)\n",
    "        #values = np.concatenate((values, values_one_mer), axis=0)\n",
    "        values_two_mer = self.extract_2_mer(sample)\n",
    "        #values = np.concatenate((values, values_two_mer), axis=0)\n",
    "        values_three_mer = self.extract_3_mer(sample)\n",
    "        #values = np.concatenate((values, values_three_mer), axis=0)\n",
    "        values = np.concatenate((values, values_one_mer, values_two_mer, \n",
    "                        values_three_mer), axis=0)\n",
    "        \n",
    "        input = torch.from_numpy(values)\n",
    "        return input, label\n",
    "    \n",
    "    def extract_1_mer(self, sample):\n",
    "        my_count = {'A': 0.0, 'C': 0.0, 'G': 0.0, 'T': 0.0}        \n",
    "        values = np.zeros((1, SAMPLE_LENGTH))\n",
    "        for i in range(SAMPLE_LENGTH):\n",
    "            my_count[sample[i]] += 1\n",
    "        \n",
    "        #for one_mer in my_count:\n",
    "        #    print(\"one mer: \", one_mer, \" : \", my_count[one_mer])\n",
    "        \n",
    "        for i in range(SAMPLE_LENGTH):\n",
    "            values[0, i] = my_count[sample[i]] / SAMPLE_LENGTH;\n",
    "        \n",
    "        #print(\"values: \", values)    \n",
    "        return values\n",
    "    \n",
    "    def extract_2_mer(self, sample):\n",
    "        my_count = {'AA': 0.0, 'AC': 0.0, 'AG': 0.0, 'AT': 0.0,\n",
    "                    'CA': 0.0, 'CC': 0.0, 'CG': 0.0, 'CT': 0.0,\n",
    "                    'GA': 0.0, 'GC': 0.0, 'GG': 0.0, 'GT': 0.0,\n",
    "                    'TA': 0.0, 'TC': 0.0, 'TG': 0.0, 'TT': 0.0} \n",
    "        values = np.zeros((2, SAMPLE_LENGTH))\n",
    "        for i in range(SAMPLE_LENGTH - 1):\n",
    "            two_mer = sample[i:i+2]\n",
    "            #print(\"two_mer: \", two_mer)\n",
    "            my_count[two_mer] += 1\n",
    "        \n",
    "        #for two_mer in my_count:\n",
    "        #    print(\"two mer: \", two_mer, \" : \", my_count[two_mer])\n",
    "        \n",
    "        values = np.zeros((2, SAMPLE_LENGTH))\n",
    "        for i in range(1,SAMPLE_LENGTH-1):\n",
    "            two_mer_left = sample[i-1:i+1]\n",
    "            two_mer_right = sample[i:i+2]\n",
    "            \n",
    "            values[0, i] = my_count[two_mer_left] / (SAMPLE_LENGTH - 1);\n",
    "            values[1, i] = my_count[two_mer_right] / (SAMPLE_LENGTH - 1);\n",
    "        \n",
    "        #print(\"values: \", values) \n",
    "        return values\n",
    "    \n",
    "    def extract_3_mer(self, sample):\n",
    "        my_count = {}\n",
    "                                        \n",
    "        for firchCh in ['A', 'C', 'G', 'T']:\n",
    "            for secondCh in ['A', 'C', 'G', 'T']:\n",
    "                for thirdCh in ['A', 'C', 'G', 'T']:\n",
    "                    three_mer = firchCh + secondCh + thirdCh\n",
    "                    my_count[three_mer] = 0.0\n",
    "        for i in range(SAMPLE_LENGTH - 2):\n",
    "            three_mer = sample[i:i+3]\n",
    "            #print(\"two_mer: \", two_mer)\n",
    "            my_count[three_mer] += 1\n",
    "                    \n",
    "        values = np.zeros((1, SAMPLE_LENGTH))\n",
    "        for i in range(1,SAMPLE_LENGTH-2):\n",
    "            three_mer = sample[i-1:i+2]\n",
    "            values[0, i] = my_count[three_mer] / SAMPLE_LENGTH;\n",
    "                    \n",
    "        return values\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return 100\n",
    "        return len(self.X)\n",
    "        \n",
    "class EnhancerCnnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnhancerCnnModel, self).__init__()\n",
    "        self.c1_1 = nn.Conv1d(8, CONV1D_FEATURE_SIZE_BLOCK1, CONV1D_KERNEL_SIZE, padding=1)\n",
    "        self.c1_1bn = nn.BatchNorm1d(CONV1D_FEATURE_SIZE_BLOCK1)\n",
    "        self.c1_2 = nn.Conv1d(CONV1D_FEATURE_SIZE_BLOCK1, CONV1D_FEATURE_SIZE_BLOCK1, \n",
    "            CONV1D_KERNEL_SIZE, padding=1)\n",
    "        self.c1_2bn = nn.BatchNorm1d(CONV1D_FEATURE_SIZE_BLOCK1)\n",
    "        self.c1_3 = nn.Conv1d(CONV1D_FEATURE_SIZE_BLOCK1, CONV1D_FEATURE_SIZE_BLOCK1, \n",
    "            CONV1D_KERNEL_SIZE, padding=1)    \n",
    "        self.c1_3bn = nn.BatchNorm1d(CONV1D_FEATURE_SIZE_BLOCK1)\n",
    "        self.p1 = nn.MaxPool1d(AVGPOOL1D_KERNEL_SIZE)\n",
    "        \n",
    "        self.c2_1 = nn.Conv1d(CONV1D_FEATURE_SIZE_BLOCK1, \n",
    "            CONV1D_FEATURE_SIZE_BLOCK2, \n",
    "            CONV1D_KERNEL_SIZE, padding=1)\n",
    "        self.c2_1bn = nn.BatchNorm1d(CONV1D_FEATURE_SIZE_BLOCK2)\n",
    "        self.c2_2 = nn.Conv1d(CONV1D_FEATURE_SIZE_BLOCK2, CONV1D_FEATURE_SIZE_BLOCK2, \n",
    "            CONV1D_KERNEL_SIZE, padding=1)\n",
    "        self.c2_2bn = nn.BatchNorm1d(CONV1D_FEATURE_SIZE_BLOCK2)\n",
    "        self.c2_3 = nn.Conv1d(CONV1D_FEATURE_SIZE_BLOCK2, CONV1D_FEATURE_SIZE_BLOCK2, \n",
    "            CONV1D_KERNEL_SIZE, padding=1)\n",
    "        self.c2_3bn = nn.BatchNorm1d(CONV1D_FEATURE_SIZE_BLOCK2)\n",
    "        self.p2 = nn.MaxPool1d(AVGPOOL1D_KERNEL_SIZE)\n",
    "        \n",
    "        self.fc = nn.Linear(768, FULLY_CONNECTED_LAYER_SIZE)        \n",
    "        self.out = nn.Linear(FULLY_CONNECTED_LAYER_SIZE, 1)\n",
    "        \n",
    "        self.criterion = nn.BCELoss()        \n",
    "     \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        # Turn (batch_size x seq_len) into (batch_size x input_size x seq_len) for CNN\n",
    "        #inputs = inputs.transpose(1,2)\n",
    "        #print(\"inputs size: \", inputs.size())        \n",
    "        output = F.relu(self.c1_1bn(self.c1_1(inputs)))\n",
    "        output = F.relu(self.c1_2bn(self.c1_2(output)))\n",
    "        output = F.relu(self.c1_3bn(self.c1_3(output)))\n",
    "        output = self.p1(output)\n",
    "        #print(\"After p1: \", output.shape) \n",
    "        \n",
    "        output = F.relu(self.c2_1bn(self.c2_1(output)))\n",
    "        output = F.relu(self.c2_2bn(self.c2_2(output)))\n",
    "        output = F.relu(self.c2_3bn(self.c2_3(output)))\n",
    "        output = self.p2(output)\n",
    "        #print(\"After p2: \", output.shape)\n",
    "        \n",
    "        output = output.view(batch_size, -1)\n",
    "        #print(\"Reshape : \", output.shape)\n",
    "        \n",
    "        output = F.relu(self.fc(output))\n",
    "        #print(\"After FC layer: \", output.shape)  \n",
    "        \n",
    "        output = torch.sigmoid(self.out(output))\n",
    "        #print(\"Final output (After sigmoid): \", output.shape)\n",
    "        #print(\"Final output: \", output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def predict_on_data(file_model, loader):\n",
    "    #model.eval()\n",
    "    model = EnhancerCnnModel()\n",
    "    #print(\"CNN Model: \", model)\n",
    "    if torch.cuda.is_available(): model.cuda()\n",
    "    \n",
    "    model.load_state_dict(torch.load(file_model))\n",
    "    model.eval()    \n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    nb_samples = 0\n",
    "    \n",
    "    arr_labels = []\n",
    "    arr_prob = []\n",
    "    \n",
    "    for i, data in enumerate(loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        #print(\"labels: \", labels)\n",
    "        \n",
    "        inputs_length = inputs.size()[0]\n",
    "        nb_samples += inputs_length\n",
    "        \n",
    "        arr_labels += labels.squeeze(1).data.cpu().numpy().tolist()\n",
    "\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            inputs = Variable(inputs.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs = Variable(inputs)\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = model.criterion(outputs, labels)\n",
    "        \n",
    "        epoch_loss = epoch_loss + loss.item() * inputs_length\n",
    "        \n",
    "        arr_prob += outputs.squeeze(1).data.cpu().numpy().tolist()\n",
    "    \n",
    "    return np.array(arr_prob)\n",
    "\n",
    "# dataset = {\"data_test\" : testset[\"data\"], \"label_test\" : testset[\"label\"]}\n",
    "def check_dataset(dataset):\n",
    "    print(\"\\n==> Checking dataset\")\n",
    "    \n",
    "    # Check data_test\n",
    "    data_test = dataset[\"data_test\"]\n",
    "    nb_error_samples = 0\n",
    "    for sample in data_test:\n",
    "        if len(sample) != 200:\n",
    "            nb_error_samples += 1\n",
    "    if nb_error_samples > 0:\n",
    "        print(\"data_teset error: \", nb_error_samples)\n",
    "    else: print(\"data_test : OK!\")\n",
    "\n",
    "def prepare_test_data(seqs):\n",
    "    label = np.zeros((len(seqs), 1))\n",
    "    testset = {\"data\" : seqs, \"label\" : label}\n",
    "    \n",
    "    return testset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_name = \"sampled_in_vae_combined_traj\"\n",
    "\n",
    "seqs = []\n",
    "with open(file_name + \".fa\", \"rt\") as f :\n",
    "    for line in f.readlines() :\n",
    "        if line[0] != \">\" :\n",
    "            seqs.append(line.strip())\n",
    "\n",
    "print(len(seqs))\n",
    "\n",
    "testset = prepare_test_data(seqs)\n",
    "\n",
    "test_dataset = EnhancerDataset(testset[\"data\"], testset[\"label\"])\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "list_model_fn = sorted(glob.glob(MODEL_DIR+\"/enhancer_*.pkl\"))\n",
    "\n",
    "y_preds = []\n",
    "for model_fn in list_model_fn:\n",
    "    y_pred = predict_on_data(model_fn, test_loader)\n",
    "    y_preds.append(y_pred.reshape(-1, 1))\n",
    "\n",
    "y_preds = np.mean(np.concatenate(y_preds, axis=-1), axis=-1)\n",
    "\n",
    "np.save(file_name + \"_iEnhancer-ECNN_detection\", y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36_fresh)",
   "language": "python",
   "name": "conda_pytorch_p36_fresh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
