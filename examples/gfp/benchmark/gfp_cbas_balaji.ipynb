{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras import layers, initializers\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import json\n",
    "#import tensorflow_probability as tfp\n",
    "\n",
    "#tfd = tfp.distributions\n",
    "\n",
    "def subselect_list(li, ixs) :\n",
    "    return [\n",
    "        li[ixs[k]] for k in range(len(ixs))\n",
    "    ]\n",
    "\n",
    "class IdentityEncoder :\n",
    "    \n",
    "    def __init__(self, seq_len, channel_map) :\n",
    "        self.seq_len = seq_len\n",
    "        self.n_channels = len(channel_map)\n",
    "        self.encode_map = channel_map\n",
    "        self.decode_map = {\n",
    "            nt: ix for ix, nt in self.encode_map.items()\n",
    "        }\n",
    "    \n",
    "    def encode(self, seq) :\n",
    "        encoding = np.zeros((self.seq_len, self.n_channels))\n",
    "        \n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "\n",
    "        return encoding\n",
    "    \n",
    "    def encode_inplace(self, seq, encoding) :\n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "    \n",
    "    def encode_inplace_sparse(self, seq, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n",
    "    \n",
    "    def decode(self, encoding) :\n",
    "        seq = ''\n",
    "    \n",
    "        for pos in range(0, encoding.shape[0]) :\n",
    "            argmax_nt = np.argmax(encoding[pos, :])\n",
    "            max_nt = np.max(encoding[pos, :])\n",
    "            seq += self.decode_map[argmax_nt]\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def decode_sparse(self, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "def contain_tf_gpu_mem_usage() :\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)\n",
    "\n",
    "contain_tf_gpu_mem_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras import layers, initializers\n",
    "from keras.models import Model, load_model\n",
    "from seqtools import SequenceTools as ST\n",
    "from util import AA, AA_IDX\n",
    "from util import build_vae\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from util import one_hot_encode_aa, partition_data, get_balaji_predictions, get_samples, get_argmax\n",
    "from util import convert_idx_array_to_aas, build_pred_vae_model, get_experimental_X_y\n",
    "from util import get_gfp_X_y_aa\n",
    "from losses import neg_log_likelihood\n",
    "\n",
    "from gfp_gp import SequenceGP\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "def build_model(M):\n",
    "    x = Input(shape=(M, 20,))\n",
    "    y = Flatten()(x)\n",
    "    y = Dense(50, activation='elu')(y)\n",
    "    y = Dense(2)(y)\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specfiy problem-specific parameters\n",
    "\n",
    "it = 1\n",
    "\n",
    "TRAIN_SIZE = 5000\n",
    "train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "num_models = [1, 5, 20][it]\n",
    "RANDOM_STATE = it + 1\n",
    "\n",
    "X_train, y_train, gt_train  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "\n",
    "L = X_train.shape[1]\n",
    "\n",
    "vae_suffix = '_%s_%i' % (train_size_str, RANDOM_STATE)\n",
    "oracle_suffix = '_%s_%i_%i' % (train_size_str, num_models, RANDOM_STATE)\n",
    "\n",
    "AA = ['a', 'r', 'n', 'd', 'c', 'q', 'e', 'g', 'h', 'i', 'l', 'k', 'm', 'f', 'p', 's', 't', 'w', 'y', 'v']\n",
    "residue_map = {key.upper() : val for val, key in enumerate(AA)}\n",
    "seq_encoder = IdentityEncoder(237, residue_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "def cbas_opt(X_train, vae_suffix, oracles, vae_0, vae_0_encoder, vae_0_decoder, weights_type='cbas',\n",
    "        LD=100, iters=20, samples=500, homoscedastic=False, homo_y_var=0.1,\n",
    "        quantile=0.95, verbose=False, alpha=1, train_gt_evals=None,\n",
    "        cutoff=1e-6, it_epochs=10, enc1_units=50, store_every=1):\n",
    "    \n",
    "    assert weights_type in ['cbas', 'rwr']\n",
    "    L = X_train.shape[1]\n",
    "    \n",
    "    vae_model = build_vae(latent_dim=20, n_tokens=20, seq_length=237, enc1_units=50)\n",
    "\n",
    "    vae_model.encoder_.load_weights(\"models/vae_0_encoder_weights%s.h5\" % vae_suffix)\n",
    "    vae_model.decoder_.load_weights(\"models/vae_0_decoder_weights%s.h5\"% vae_suffix)\n",
    "    vae_model.vae_.load_weights(\"models/vae_0_vae_weights%s.h5\"% vae_suffix)\n",
    "    vae = vae_model.vae_\n",
    "    vae_encoder = vae_model.encoder_\n",
    "    vae_decoder = vae_model.decoder_\n",
    "    \n",
    "    def get_samples(Xt_p):\n",
    "        Xt_sampled = np.zeros_like(Xt_p)\n",
    "        for i in range(Xt_p.shape[0]):\n",
    "            for j in range(Xt_p.shape[1]):\n",
    "                p = Xt_p[i, j, :]\n",
    "                k = np.random.choice(range(len(p)), p=p)\n",
    "                Xt_sampled[i, j, k] = 1.\n",
    "        return Xt_sampled\n",
    "\n",
    "    generated_sequences = []\n",
    "    n_top = 0\n",
    "    y_star = -np.inf\n",
    "    \n",
    "    for t in range(iters):\n",
    "        ### Take Samples ###\n",
    "        zt = np.random.randn(samples, LD)\n",
    "        zt_dummy = np.zeros((samples, 1))\n",
    "        if t > 0:\n",
    "            Xt_p = vae_decoder.predict([zt])\n",
    "            Xt = get_samples(Xt_p)\n",
    "        else:\n",
    "            Xt = X_train\n",
    "        \n",
    "        ### Evaluate oracle ###\n",
    "        yt, yt_var = get_balaji_predictions(oracles, Xt)\n",
    "        \n",
    "        ### Calculate weights ###\n",
    "        if t > 0:\n",
    "            if weights_type == 'cbas': \n",
    "                log_pxt = np.sum(np.log(Xt_p) * Xt, axis=(1, 2))\n",
    "                X0_p = vae_0_decoder.predict([zt])\n",
    "                log_px0 = np.sum(np.log(X0_p) * Xt, axis=(1, 2))\n",
    "                w1 = np.exp(log_px0-log_pxt)\n",
    "                y_star_1 = np.percentile(yt, quantile*100)\n",
    "                if y_star_1 > y_star:\n",
    "                    y_star = y_star_1\n",
    "                w2= scipy.stats.norm.sf(y_star, loc=yt, scale=np.sqrt(yt_var))\n",
    "                weights = w1*w2\n",
    "            elif weights_type == 'rwr':\n",
    "                weights = np.exp(alpha*yt)\n",
    "                weights /= np.sum(weights)\n",
    "                weights *= Xt.shape[0]\n",
    "        else:\n",
    "            weights = np.ones(yt.shape[0])\n",
    "            \n",
    "        if t % store_every == 0 :\n",
    "            Xt_seqs = []\n",
    "            AA = ['a', 'r', 'n', 'd', 'c', 'q', 'e', 'g', 'h', 'i', 'l', 'k', 'm', 'f', 'p', 's', 't', 'w', 'y', 'v']\n",
    "            nt_map_inv = {key : val.upper() for key, val in enumerate(AA)}\n",
    "            \n",
    "            for i in range(Xt.shape[0]) :\n",
    "                xt_seq = ''\n",
    "                for j in range(Xt.shape[1]) :\n",
    "                    argmax_j = np.argmax(Xt[i, j, :])\n",
    "                    xt_seq += nt_map_inv[argmax_j]\n",
    "                \n",
    "                Xt_seqs.append(xt_seq)\n",
    "            \n",
    "            generated_sequences.append(Xt_seqs)\n",
    "        \n",
    "        if verbose:\n",
    "            print(weights_type.upper(), t, np.median(yt))\n",
    "        \n",
    "        ### Train model ###\n",
    "        if t == 0:\n",
    "            vae_encoder.set_weights(vae_0_encoder.get_weights())\n",
    "            vae_decoder.set_weights(vae_0_decoder.get_weights())\n",
    "            vae.set_weights(vae_0.get_weights())\n",
    "        else:\n",
    "            cutoff_idx = np.where(weights < cutoff)\n",
    "            Xt = np.delete(Xt, cutoff_idx, axis=0)\n",
    "            yt = np.delete(yt, cutoff_idx, axis=0)\n",
    "            weights = np.delete(weights, cutoff_idx, axis=0)\n",
    "            \n",
    "            # train the autoencoder\n",
    "            _ = vae_model.fit(\n",
    "                [Xt], [Xt, np.zeros(Xt.shape[0])],\n",
    "                shuffle=False,\n",
    "                sample_weight=[weights, weights],\n",
    "                epochs=it_epochs,\n",
    "                batch_size=32,\n",
    "                verbose=0\n",
    "            )\n",
    "    \n",
    "    return generated_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load predictor\n",
    "\n",
    "oracles = [build_model(L) for i in range(num_models)]\n",
    "for i in range(num_models) :\n",
    "    oracles[i].load_weights(\"models/oracle_%i%s.h5\" % (i, oracle_suffix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load models\n",
    "\n",
    "vae_0 = build_vae(latent_dim=20, n_tokens=20, seq_length=237, enc1_units=50)\n",
    "\n",
    "vae_0.encoder_.load_weights(\"models/vae_0_encoder_weights%s.h5\" % vae_suffix)\n",
    "vae_0.decoder_.load_weights(\"models/vae_0_decoder_weights%s.h5\"% vae_suffix)\n",
    "vae_0.vae_.load_weights(\"models/vae_0_vae_weights%s.h5\"% vae_suffix)\n",
    "\n",
    "vae_0_encoder = vae_0.encoder_\n",
    "vae_0_decoder = vae_0.decoder_\n",
    "vae_0_vae = vae_0.vae_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "vae_prefix_str = \"\"\n",
    "\n",
    "weights_type = 'cbas'\n",
    "run_ix = 0\n",
    "n_epochs = 150\n",
    "n_samples = 1000\n",
    "quantile = 0.8\n",
    "alpha = 1.0\n",
    "it_epochs = 10\n",
    "\n",
    "generated_sequences = cbas_opt(X_train, vae_suffix, oracles, vae_0_vae, vae_0_encoder, vae_0_decoder,\n",
    "        LD=20, iters=n_epochs, samples=n_samples, weights_type=weights_type, alpha=alpha,\n",
    "        quantile=quantile, verbose=True, cutoff=1e-6, it_epochs=it_epochs, store_every=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_name = \"gfp_\" + weights_type + \"_weak_balaji_vae\" + vae_prefix_str + \"_iters_\" + str(n_epochs) + \"_samples_\" + str(n_samples) + \"_q_\" + str(quantile).replace(\".\", \"\") + \"_alpha_\" + str(alpha).replace(\".\", \"\") + \"_it_epochs_\" + str(it_epochs) + \"_run_\" + str(run_ix)\n",
    "\n",
    "if not os.path.isdir('cbas_weak_balaji/' + experiment_name):\n",
    "    os.makedirs('cbas_weak_balaji/' + experiment_name)\n",
    "\n",
    "for epoch_i in range(n_epochs) :\n",
    "    with open('cbas_weak_balaji/' + experiment_name + \"/\" + \"iter_\" + str(epoch_i) + '.txt', 'wt') as f :\n",
    "        for seq in generated_sequences[epoch_i] :\n",
    "            f.write(seq + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "vae_prefix_str = \"\"\n",
    "\n",
    "weights_type = 'cbas'\n",
    "run_ix = 1\n",
    "n_epochs = 150\n",
    "n_samples = 1000\n",
    "quantile = 0.8\n",
    "alpha = 1.0\n",
    "it_epochs = 10\n",
    "\n",
    "generated_sequences = cbas_opt(X_train, vae_suffix, oracles, vae_0_vae, vae_0_encoder, vae_0_decoder,\n",
    "        LD=20, iters=n_epochs, samples=n_samples, weights_type=weights_type, alpha=alpha,\n",
    "        quantile=quantile, verbose=True, cutoff=1e-6, it_epochs=it_epochs, store_every=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_name = \"gfp_\" + weights_type + \"_weak_balaji_vae\" + vae_prefix_str + \"_iters_\" + str(n_epochs) + \"_samples_\" + str(n_samples) + \"_q_\" + str(quantile).replace(\".\", \"\") + \"_alpha_\" + str(alpha).replace(\".\", \"\") + \"_it_epochs_\" + str(it_epochs) + \"_run_\" + str(run_ix)\n",
    "\n",
    "if not os.path.isdir('cbas_weak_balaji/' + experiment_name):\n",
    "    os.makedirs('cbas_weak_balaji/' + experiment_name)\n",
    "\n",
    "for epoch_i in range(n_epochs) :\n",
    "    with open('cbas_weak_balaji/' + experiment_name + \"/\" + \"iter_\" + str(epoch_i) + '.txt', 'wt') as f :\n",
    "        for seq in generated_sequences[epoch_i] :\n",
    "            f.write(seq + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "vae_prefix_str = \"\"\n",
    "\n",
    "weights_type = 'cbas'\n",
    "run_ix = 2\n",
    "n_epochs = 150\n",
    "n_samples = 1000\n",
    "quantile = 0.8\n",
    "alpha = 1.0\n",
    "it_epochs = 10\n",
    "\n",
    "generated_sequences = cbas_opt(X_train, vae_suffix, oracles, vae_0_vae, vae_0_encoder, vae_0_decoder,\n",
    "        LD=20, iters=n_epochs, samples=n_samples, weights_type=weights_type, alpha=alpha,\n",
    "        quantile=quantile, verbose=True, cutoff=1e-6, it_epochs=it_epochs, store_every=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_name = \"gfp_\" + weights_type + \"_weak_balaji_vae\" + vae_prefix_str + \"_iters_\" + str(n_epochs) + \"_samples_\" + str(n_samples) + \"_q_\" + str(quantile).replace(\".\", \"\") + \"_alpha_\" + str(alpha).replace(\".\", \"\") + \"_it_epochs_\" + str(it_epochs) + \"_run_\" + str(run_ix)\n",
    "\n",
    "if not os.path.isdir('cbas_weak_balaji/' + experiment_name):\n",
    "    os.makedirs('cbas_weak_balaji/' + experiment_name)\n",
    "\n",
    "for epoch_i in range(n_epochs) :\n",
    "    with open('cbas_weak_balaji/' + experiment_name + \"/\" + \"iter_\" + str(epoch_i) + '.txt', 'wt') as f :\n",
    "        for seq in generated_sequences[epoch_i] :\n",
    "            f.write(seq + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
