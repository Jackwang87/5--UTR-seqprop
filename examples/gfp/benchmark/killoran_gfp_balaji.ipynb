{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras import layers, initializers\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import json\n",
    "#import tensorflow_probability as tfp\n",
    "\n",
    "#tfd = tfp.distributions\n",
    "\n",
    "def subselect_list(li, ixs) :\n",
    "    return [\n",
    "        li[ixs[k]] for k in range(len(ixs))\n",
    "    ]\n",
    "\n",
    "class IdentityEncoder :\n",
    "    \n",
    "    def __init__(self, seq_len, channel_map) :\n",
    "        self.seq_len = seq_len\n",
    "        self.n_channels = len(channel_map)\n",
    "        self.encode_map = channel_map\n",
    "        self.decode_map = {\n",
    "            nt: ix for ix, nt in self.encode_map.items()\n",
    "        }\n",
    "    \n",
    "    def encode(self, seq) :\n",
    "        encoding = np.zeros((self.seq_len, self.n_channels))\n",
    "        \n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "\n",
    "        return encoding\n",
    "    \n",
    "    def encode_inplace(self, seq, encoding) :\n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "    \n",
    "    def encode_inplace_sparse(self, seq, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n",
    "    \n",
    "    def decode(self, encoding) :\n",
    "        seq = ''\n",
    "    \n",
    "        for pos in range(0, encoding.shape[0]) :\n",
    "            argmax_nt = np.argmax(encoding[pos, :])\n",
    "            max_nt = np.max(encoding[pos, :])\n",
    "            seq += self.decode_map[argmax_nt]\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def decode_sparse(self, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "def contain_tf_gpu_mem_usage() :\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)\n",
    "\n",
    "contain_tf_gpu_mem_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras import layers, initializers\n",
    "from keras.models import Model, load_model\n",
    "from seqtools import SequenceTools as ST\n",
    "from util import AA, AA_IDX\n",
    "from util import build_vae\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from util import one_hot_encode_aa, partition_data, get_balaji_predictions, get_samples, get_argmax\n",
    "from util import convert_idx_array_to_aas, build_pred_vae_model, get_experimental_X_y\n",
    "from util import get_gfp_X_y_aa\n",
    "from losses import neg_log_likelihood\n",
    "\n",
    "from gfp_gp import SequenceGP\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "def build_model(M):\n",
    "    x = Input(shape=(M, 20,))\n",
    "    y = Flatten()(x)\n",
    "    y = Dense(50, activation='elu')(y)\n",
    "    y = Dense(2)(y)\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specfiy problem-specific parameters\n",
    "\n",
    "it = 1\n",
    "\n",
    "TRAIN_SIZE = 5000\n",
    "train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "num_models = [1, 5, 20][it]\n",
    "RANDOM_STATE = it + 1\n",
    "\n",
    "X_train, y_train, gt_train  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "\n",
    "L = X_train.shape[1]\n",
    "\n",
    "vae_suffix = '_%s_%i' % (train_size_str, RANDOM_STATE)\n",
    "oracle_suffix = '_%s_%i_%i' % (train_size_str, num_models, RANDOM_STATE)\n",
    "\n",
    "AA = ['a', 'r', 'n', 'd', 'c', 'q', 'e', 'g', 'h', 'i', 'l', 'k', 'm', 'f', 'p', 's', 't', 'w', 'y', 'v']\n",
    "residue_map = {key.upper() : val for val, key in enumerate(AA)}\n",
    "seq_encoder = IdentityEncoder(237, residue_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def killoran_opt(vae_decoder, oracles,\n",
    "                 steps=20000, store_every1=5, store_every2=100, store_swap_iter=100, epsilon1=10**-5, epsilon2=1, noise_std=10**-5, save_path=None,\n",
    "                 LD=100, verbose=False, adam=False):\n",
    "    \n",
    "    G = vae_decoder\n",
    "    f = oracles\n",
    "    \n",
    "    sess = K.get_session()\n",
    "    zt = K.tf.Variable(np.random.normal(size=[1, LD]), dtype='float32')\n",
    "    zt_dummy = K.tf.Variable(np.zeros((1, 1)), trainable=False, dtype='float32')\n",
    "    \n",
    "    pred_input = K.tf.Variable(np.zeros((1, 237, 20)), dtype='float32')\n",
    "    \n",
    "    gen_output = G([zt])\n",
    "    #prior = tfd.Normal(0, 1)\n",
    "    #p_z = prior.log_prob(zt)\n",
    "    \n",
    "    predictions = K.tf.reduce_mean([f[i]([pred_input])[0, 0] for i in range(len(f))])\n",
    "    update_pred_input = K.tf.assign(pred_input, gen_output)\n",
    "    dfdx = K.tf.gradients(ys=-predictions, xs=pred_input)[0]\n",
    "    dfdz = K.tf.gradients(gen_output, zt, grad_ys=dfdx)[0]\n",
    "    #dpz = K.tf.gradients(p_z, zt)[0]\n",
    "    \n",
    "    noise = K.tf.random_normal(shape=[1, LD], stddev=noise_std)\n",
    "    eps1 = K.tf.Variable(epsilon1, trainable=False)\n",
    "    eps2 = K.tf.Variable(epsilon2, trainable=False)\n",
    "    if adam:\n",
    "        optimizer = K.tf.train.AdamOptimizer(learning_rate=epsilon2)\n",
    "        step = dfdz + noise\n",
    "    else:\n",
    "        optimizer = K.tf.train.GradientDescentOptimizer(learning_rate=1)\n",
    "        step = eps1 * dpz + eps2 * dfdz + noise\n",
    "    \n",
    "    design_op = optimizer.apply_gradients([(step, zt)])\n",
    "    adam_initializers = [var.initializer for var in K.tf.global_variables() if 'Adam' in var.name or 'beta' in var.name]\n",
    "    sess.run(adam_initializers)\n",
    "    sess.run(pred_input.initializer)\n",
    "    sess.run(zt.initializer)\n",
    "    sess.run(eps1.initializer)\n",
    "    sess.run(eps2.initializer)\n",
    "    \n",
    "    sess.run(zt_dummy.initializer)\n",
    "    sess.run(pred_input.initializer)\n",
    "    \n",
    "    s = sess.run(K.tf.shape(zt))\n",
    "    sess.run(update_pred_input, {\n",
    "        zt: np.random.normal(size=s),\n",
    "        zt_dummy: np.zeros((1, 1))\n",
    "    })\n",
    "    z_0 = sess.run([zt])\n",
    "    \n",
    "    store_every = store_every1\n",
    "    \n",
    "    xt_prev = None\n",
    "    for t in range(steps):\n",
    "        if t % 1000 == 0 :\n",
    "            print(\"Running step \" + str(t) + \"...\")\n",
    "        \n",
    "        if t > store_swap_iter :\n",
    "            store_every = store_every2\n",
    "        \n",
    "        xt0, _, = sess.run([gen_output, design_op], {eps1: epsilon1, eps2:epsilon2})\n",
    "        pred_in, preds = sess.run([update_pred_input, predictions])\n",
    "        \n",
    "        AA = ['a', 'r', 'n', 'd', 'c', 'q', 'e', 'g', 'h', 'i', 'l', 'k', 'm', 'f', 'p', 's', 't', 'w', 'y', 'v']\n",
    "        nt_map_inv = {key : val.upper() for key, val in enumerate(AA)}\n",
    "        \n",
    "        xt_seq = ''\n",
    "        for j in range(xt0.shape[1]) :\n",
    "            argmax_j = np.argmax(xt0[0, j, :])\n",
    "            xt_seq += nt_map_inv[argmax_j]\n",
    "        \n",
    "        if save_path is not None and t % store_every == 0 :\n",
    "            with open(save_path + \"_iter_\" + str(t) + \".txt\", \"a+\") as f :\n",
    "                f.write(xt_seq + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_killoran(n_traj=5, steps=20000, vae_prefix_str=\"\", vae_suffix=vae_suffix, oracle_suffix=oracle_suffix):\n",
    "    \n",
    "    for i in range(n_traj):\n",
    "        RANDOM_STATE = i+1\n",
    "        print(RANDOM_STATE)\n",
    "        \n",
    "        sess = tf.Session(graph=tf.get_default_graph())\n",
    "        K.set_session(sess)\n",
    "        \n",
    "        #Load models\n",
    "        oracles = [build_model(L) for i in range(num_models)]\n",
    "        for i in range(num_models) :\n",
    "            oracles[i].load_weights(\"models/oracle_%i%s.h5\" % (i, oracle_suffix))\n",
    "        \n",
    "        vae_0 = build_vae(latent_dim=20, n_tokens=20, seq_length=237, enc1_units=50)\n",
    "\n",
    "        vae_0.encoder_.load_weights(\"models/vae_0_encoder_weights%s.h5\" % vae_suffix)\n",
    "        vae_0.decoder_.load_weights(\"models/vae_0_decoder_weights%s.h5\"% vae_suffix)\n",
    "        vae_0.vae_.load_weights(\"models/vae_0_vae_weights%s.h5\"% vae_suffix)\n",
    "\n",
    "        #Load decoder model\n",
    "        vae_0.decoder_.trainable = False\n",
    "        vae_0.decoder_.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999))\n",
    "        decoder = vae_0.decoder_\n",
    "\n",
    "        killoran_opt(decoder, oracles, steps=steps, epsilon1=0., epsilon2=0.1,  \n",
    "                                     noise_std=1e-6, store_every1=5, store_every2=100, store_swap_iter=100,\n",
    "                                     LD=20, verbose=False, adam=True,\n",
    "                                     save_path='killoran/killoran_weak_balaji_vae' + vae_prefix_str + '_gfp_seqs'\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Running step 0...\n",
      "Running step 1000...\n",
      "Running step 2000...\n",
      "Running step 3000...\n",
      "Running step 4000...\n",
      "2\n",
      "Running step 0...\n",
      "Running step 1000...\n",
      "Running step 2000...\n",
      "Running step 3000...\n",
      "Running step 4000...\n",
      "3\n",
      "Running step 0...\n",
      "Running step 1000...\n",
      "Running step 2000...\n",
      "Running step 3000...\n",
      "Running step 4000...\n",
      "4\n",
      "Running step 0...\n",
      "Running step 1000...\n",
      "Running step 2000...\n",
      "Running step 3000...\n",
      "Running step 4000...\n",
      "5\n",
      "Running step 0...\n",
      "Running step 1000...\n",
      "Running step 2000...\n",
      "Running step 3000...\n",
      "Running step 4000...\n",
      "6\n",
      "Running step 0...\n",
      "Running step 1000...\n",
      "Running step 2000...\n",
      "Running step 3000...\n",
      "Running step 4000...\n",
      "7\n",
      "Running step 0...\n",
      "Running step 1000...\n",
      "Running step 2000...\n",
      "Running step 3000...\n",
      "Running step 4000...\n",
      "8\n",
      "Running step 0...\n",
      "Running step 1000...\n",
      "Running step 2000...\n",
      "Running step 3000...\n",
      "Running step 4000...\n",
      "9\n",
      "Running step 0...\n",
      "Running step 1000...\n",
      "Running step 2000...\n",
      "Running step 3000...\n",
      "Running step 4000...\n",
      "10\n",
      "Running step 0...\n",
      "Running step 1000...\n",
      "Running step 2000...\n",
      "Running step 3000...\n",
      "Running step 4000...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_killoran(n_traj=10, steps=5000, vae_prefix_str=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
