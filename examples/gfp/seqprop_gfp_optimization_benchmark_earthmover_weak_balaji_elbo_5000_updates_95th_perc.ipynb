{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, ConvLSTM2D, GRU, BatchNormalization, LocallyConnected2D, Permute\n",
    "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import keras.losses\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import isolearn.keras as iso\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as spio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import isolearn.keras as iso\n",
    "\n",
    "from seqprop.visualization import *\n",
    "from seqprop_generator_protein_layer_norm import *\n",
    "from seqprop.predictor import *\n",
    "from seqprop.optimizer import *\n",
    "\n",
    "from definitions.gfp_weak_balaji import load_saved_predictor\n",
    "\n",
    "class IdentityEncoder(iso.SequenceEncoder) :\n",
    "    \n",
    "    def __init__(self, seq_len, channel_map) :\n",
    "        super(IdentityEncoder, self).__init__('identity', (seq_len, len(channel_map)))\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.n_channels = len(channel_map)\n",
    "        self.encode_map = channel_map\n",
    "        self.decode_map = {\n",
    "            nt: ix for ix, nt in self.encode_map.items()\n",
    "        }\n",
    "    \n",
    "    def encode(self, seq) :\n",
    "        encoding = np.zeros((self.seq_len, self.n_channels))\n",
    "        \n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "\n",
    "        return encoding\n",
    "    \n",
    "    def encode_inplace(self, seq, encoding) :\n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "    \n",
    "    def encode_inplace_sparse(self, seq, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n",
    "    \n",
    "    def decode(self, encoding) :\n",
    "        seq = ''\n",
    "    \n",
    "        for pos in range(0, encoding.shape[0]) :\n",
    "            argmax_nt = np.argmax(encoding[pos, :])\n",
    "            max_nt = np.max(encoding[pos, :])\n",
    "            seq += self.decode_map[argmax_nt]\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def decode_sparse(self, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "def contain_tf_gpu_mem_usage() :\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)\n",
    "\n",
    "contain_tf_gpu_mem_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras import layers, initializers\n",
    "from keras.models import Model, load_model\n",
    "from seqtools import SequenceTools as ST\n",
    "from util import AA, AA_IDX\n",
    "from util import build_vae\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from util import one_hot_encode_aa, partition_data, get_balaji_predictions, get_samples, get_argmax\n",
    "from util import convert_idx_array_to_aas, build_pred_vae_model, get_experimental_X_y\n",
    "from util import get_gfp_X_y_aa\n",
    "from losses import neg_log_likelihood\n",
    "\n",
    "def build_model(M):\n",
    "    x = Input(shape=(M, 20,))\n",
    "    y = Flatten()(x)\n",
    "    y = Dense(50, activation='elu')(y)\n",
    "    y = Dense(2)(y)\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def get_z_sample_numpy(z_mean, z_log_var, n_samples=1) :\n",
    "    \n",
    "    n = z_mean.shape[0]\n",
    "    m = z_mean.shape[2]\n",
    "    \n",
    "    epsilon = np.random.normal(loc=0., scale=1., size=(n, n_samples, m))\n",
    "    \n",
    "    return z_mean + np.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "#Evaluate VAE Likelihood (ELBO) on supplied data\n",
    "def evaluate_elbo(vae_encoder_model, vae_decoder_model, sequence_one_hots, pwm_start=0, pwm_end=-1, n_samples=1, decoded_pwm_eps=1e-6) :\n",
    "    _epsilon = 10**-6\n",
    "    \n",
    "    if pwm_end == -1 :\n",
    "        pwm_end = sequence_one_hots.shape[2]\n",
    "    \n",
    "    #Get sequence VAE encodings\n",
    "    z_mean, z_log_var = vae_encoder_model.predict(x=sequence_one_hots, batch_size=32, verbose=False)\n",
    "\n",
    "    z_mean = np.tile(np.expand_dims(z_mean, axis=1), (1, n_samples, 1))\n",
    "    z_log_var = np.tile(np.expand_dims(z_log_var, axis=1), (1, n_samples, 1))\n",
    "    z = get_z_sample_numpy(z_mean, z_log_var, n_samples=n_samples)\n",
    "    \n",
    "    #Get re-decoded sequence PWMs\n",
    "    decoded_pwms = np.zeros((sequence_one_hots.shape[0], n_samples) + sequence_one_hots.shape[1:])\n",
    "\n",
    "    for sample_ix in range(n_samples) :\n",
    "        decoded_pwms[:, sample_ix, :, :] = vae_decoder_model.predict(x=z[:, sample_ix, :], batch_size=32, verbose=False)\n",
    "\n",
    "    decoded_pwms = np.clip(decoded_pwms, decoded_pwm_eps, 1. - decoded_pwm_eps)\n",
    "    \n",
    "    sequence_one_hots_expanded = np.tile(np.expand_dims(sequence_one_hots, axis=1), (1, n_samples, 1, 1))\n",
    "    \n",
    "    #Calculate reconstruction log prob\n",
    "    log_p_x_given_z = np.sum(np.sum(sequence_one_hots_expanded[:, :, pwm_start:pwm_end, :] * np.log(np.clip(decoded_pwms[:, :, pwm_start:pwm_end, :], _epsilon, 1. - _epsilon)) / np.log(10.), axis=3), axis=2)\n",
    "\n",
    "    #Calculate standard normal and importance log probs\n",
    "    log_p_std_normal = np.sum(norm.logpdf(z, 0., 1.) / np.log(10.), axis=-1)\n",
    "    log_p_importance = np.sum(norm.logpdf(z, z_mean, np.sqrt(np.exp(z_log_var))) / np.log(10.), axis=-1)\n",
    "\n",
    "    #Calculate per-sample ELBO\n",
    "    log_p_vae = log_p_x_given_z + log_p_std_normal - log_p_importance\n",
    "    log_p_vae_div_n = log_p_vae - np.log(n_samples) / np.log(10.)\n",
    "\n",
    "    #Calculate mean ELBO across samples (log-sum-exp trick)\n",
    "    max_log_p_vae = np.max(log_p_vae_div_n, axis=-1)\n",
    "    \n",
    "    log_mean_p_vae = max_log_p_vae + np.log(np.sum(10**(log_p_vae_div_n - np.expand_dims(max_log_p_vae, axis=-1)), axis=-1)) / np.log(10.)\n",
    "    mean_log_p_vae = np.mean(log_mean_p_vae)\n",
    "    \n",
    "    return log_mean_p_vae, mean_log_p_vae, log_p_vae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean log(likelihood) = -0.8908\n",
      "mode log(likelihood) = -0.1512\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Evaluate ELBO distribution on GFP dataset, decoder epislon = 1e-6\n",
    "\n",
    "n_z_samples = 128\n",
    "\n",
    "it = 0\n",
    "TRAIN_SIZE = 5000\n",
    "train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "num_models = [1, 5, 20][it]\n",
    "RANDOM_STATE = it + 1\n",
    "\n",
    "X_train, y_train, gt_train  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "\n",
    "L = X_train.shape[1]\n",
    "\n",
    "vae_suffix = '_%s_%i' % (train_size_str, RANDOM_STATE)\n",
    "\n",
    "vae_0 = build_vae(latent_dim=20, n_tokens=20, seq_length=L, enc1_units=50)\n",
    "\n",
    "vae_0.encoder_.load_weights(\"models/vae_0_encoder_weights%s.h5\" % vae_suffix)\n",
    "vae_0.decoder_.load_weights(\"models/vae_0_decoder_weights%s.h5\"% vae_suffix)\n",
    "vae_0.vae_.load_weights(\"models/vae_0_vae_weights%s.h5\"% vae_suffix)\n",
    "\n",
    "#Compute multi-sample ELBO on test set\n",
    "log_mean_p_vae_test, mean_log_p_vae_test, log_p_vae_test = evaluate_elbo(vae_0.encoder_, vae_0.decoder_, X_train, n_samples=n_z_samples)\n",
    "\n",
    "#Log Likelihood Plot\n",
    "plot_min_val = None\n",
    "plot_max_val = None\n",
    "\n",
    "f = plt.figure(figsize=(6, 4))\n",
    "\n",
    "log_p_vae_test_hist, log_p_vae_test_edges = np.histogram(log_mean_p_vae_test, bins=50, density=True)\n",
    "bin_width_test = log_p_vae_test_edges[1] - log_p_vae_test_edges[0]\n",
    "\n",
    "mean_log_p_vae_test = np.mean(log_mean_p_vae_test)\n",
    "mode_log_p_vae_test = log_p_vae_test_edges[np.argmax(log_p_vae_test_hist)] + bin_width_test / 2.\n",
    "\n",
    "print(\"mean log(likelihood) = \" + str(round(mean_log_p_vae_test, 4)))\n",
    "print(\"mode log(likelihood) = \" + str(round(mode_log_p_vae_test, 4)))\n",
    "\n",
    "\n",
    "plt.bar(log_p_vae_test_edges[1:] - bin_width_test/2., log_p_vae_test_hist, width=bin_width_test, linewidth=2, edgecolor='black', color='orange')\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "if plot_min_val is not None and plot_max_val is not None :\n",
    "    plt.xlim(plot_min_val, plot_max_val)\n",
    "\n",
    "plt.xlabel(\"VAE Log Likelihood\", fontsize=14)\n",
    "plt.ylabel(\"Data Density\", fontsize=14)\n",
    "\n",
    "plt.axvline(x=mean_log_p_vae_test, linewidth=2, color='red', linestyle=\"--\")\n",
    "plt.axvline(x=mode_log_p_vae_test, linewidth=2, color='purple', linestyle=\"--\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specfiy problem-specific parameters\n",
    "\n",
    "it = 1\n",
    "\n",
    "TRAIN_SIZE = 5000\n",
    "train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "num_models = [1, 5, 20][it]\n",
    "RANDOM_STATE = it + 1\n",
    "\n",
    "X_train, y_train, gt_train  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "\n",
    "L = X_train.shape[1]\n",
    "\n",
    "vae_suffix = '_%s_%i' % (train_size_str, RANDOM_STATE)\n",
    "oracle_suffix = '_%s_%i_%i' % (train_size_str, num_models, RANDOM_STATE)\n",
    "\n",
    "vae_latent_dim = 20\n",
    "\n",
    "vae_pwm_start = 0\n",
    "vae_pwm_end = 237\n",
    "\n",
    "#VAE parameter collection\n",
    "vae_params = [\n",
    "    \"models/\",\n",
    "    vae_suffix,\n",
    "    vae_latent_dim,\n",
    "    vae_pwm_start,\n",
    "    vae_pwm_end\n",
    "]\n",
    "\n",
    "AA = ['a', 'r', 'n', 'd', 'c', 'q', 'e', 'g', 'h', 'i', 'l', 'k', 'm', 'f', 'p', 's', 't', 'w', 'y', 'v']\n",
    "residue_map = {key.upper() : val for val, key in enumerate(AA)}\n",
    "seq_encoder = IdentityEncoder(237, residue_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "\n",
    "#Stochastic Binarized Neuron helper functions (Tensorflow)\n",
    "#ST Estimator code adopted from https://r2rt.com/beyond-binary-ternary-and-one-hot-neurons.html\n",
    "#See Github https://github.com/spitis/\n",
    "\n",
    "def st_sampled_softmax(logits):\n",
    "    with ops.name_scope(\"STSampledSoftmax\") as namescope :\n",
    "        nt_probs = tf.nn.softmax(logits)\n",
    "        onehot_dim = logits.get_shape().as_list()[1]\n",
    "        sampled_onehot = tf.one_hot(tf.squeeze(tf.multinomial(logits, 1), 1), onehot_dim, 1.0, 0.0)\n",
    "        with tf.get_default_graph().gradient_override_map({'Ceil': 'Identity', 'Mul': 'STMul'}):\n",
    "            return tf.ceil(sampled_onehot * nt_probs)\n",
    "\n",
    "def st_hardmax_softmax(logits):\n",
    "    with ops.name_scope(\"STHardmaxSoftmax\") as namescope :\n",
    "        nt_probs = tf.nn.softmax(logits)\n",
    "        onehot_dim = logits.get_shape().as_list()[1]\n",
    "        sampled_onehot = tf.one_hot(tf.argmax(nt_probs, 1), onehot_dim, 1.0, 0.0)\n",
    "        with tf.get_default_graph().gradient_override_map({'Ceil': 'Identity', 'Mul': 'STMul'}):\n",
    "            return tf.ceil(sampled_onehot * nt_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper dummy function for loading keras models\n",
    "def min_pred(y_true, y_pred) :\n",
    "        return y_pred\n",
    "\n",
    "#Keras function to calculate normal distribution log pdf\n",
    "def normal_log_prob(x, loc=0., scale=1.) :\n",
    "    return _normal_log_unnormalized_prob(x, loc, scale) - _normal_log_normalization(scale)\n",
    "\n",
    "def _normal_log_unnormalized_prob(x, loc, scale):\n",
    "    return -0.5 * K.square((x - loc) / scale)\n",
    "\n",
    "def _normal_log_normalization(scale):\n",
    "    return 0.5 * K.log(2. * K.constant(np.pi)) + K.log(scale)\n",
    "\n",
    "#Keras function to sample latent vectors\n",
    "def get_z_sample(z_inputs) :\n",
    "    \n",
    "    z_mean, z_log_var = z_inputs\n",
    "    \n",
    "    batch_size = K.shape(z_mean)[0]\n",
    "    latent_dim = K.int_shape(z_mean)[1]\n",
    "    \n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim))\n",
    "    \n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "#Keras function to sample (multiple) latent vectors\n",
    "def get_z_samples(z_inputs, n_z_samples=1) :\n",
    "    \n",
    "    z_mean, z_log_var = z_inputs\n",
    "    \n",
    "    batch_size = K.shape(z_mean)[0]\n",
    "    n_samples = K.shape(z_mean)[1]\n",
    "    latent_dim = K.int_shape(z_mean)[3]\n",
    "    \n",
    "    epsilon = K.random_normal(shape=(batch_size, n_samples, n_z_samples, latent_dim))\n",
    "    \n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "#Code for constructing a (differentiable) VAE ELBO estimator in Keras\n",
    "def build_sqp_vae(generator, vae_path, vae_suffix, batch_size=1, seq_length=237, n_samples=1, n_z_samples=1, vae_latent_dim=100, vae_pwm_start=0, vae_pwm_end=-1, transform_adversary=False) :\n",
    "\n",
    "    #Connect generated sequence samples from generator to vae\n",
    "    generated_sequence_pwm = generator.outputs[1]\n",
    "    generated_sequence_samples = generator.outputs[2]\n",
    "    \n",
    "    if vae_pwm_end == -1 :\n",
    "        vae_pwm_end = seq_length\n",
    "    \n",
    "    vae_0 = build_vae(latent_dim=20, n_tokens=20, seq_length=seq_length, enc1_units=50)\n",
    "\n",
    "    vae_0.encoder_.load_weights(vae_path + \"vae_0_encoder_weights%s.h5\" % vae_suffix)\n",
    "    vae_0.decoder_.load_weights(vae_path + \"vae_0_decoder_weights%s.h5\"% vae_suffix)\n",
    "    vae_0.vae_.load_weights(vae_path + \"vae_0_vae_weights%s.h5\"% vae_suffix)\n",
    "    \n",
    "    #Freeze encoder model\n",
    "    vae_0.encoder_.trainable = False\n",
    "    vae_0.encoder_.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999))\n",
    "    saved_vae_encoder_model = vae_0.encoder_\n",
    "    \n",
    "    #Load decoder model\n",
    "    vae_0.decoder_.trainable = False\n",
    "    vae_0.decoder_.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999))\n",
    "    saved_vae_decoder_model = vae_0.decoder_\n",
    "    \n",
    "    #Construct vae elbo keras function (lambda layer)\n",
    "    def _vae_elbo_func(pwm_and_sampled_pwm, batch_size=batch_size, n_samples=n_samples, n_z_samples=n_z_samples) :\n",
    "        \n",
    "        pwm_1, sampled_pwm_1 = pwm_and_sampled_pwm\n",
    "        \n",
    "        def _encode_and_sample(saved_vae_encoder_model, pwm, sampled_pwm, vae_pwm_start, vae_pwm_end, vae_latent_dim, n_z_samples) :\n",
    "            vae_pwm = pwm[:, vae_pwm_start:vae_pwm_end, :, :]\n",
    "            vae_sampled_pwm = sampled_pwm[:, :, vae_pwm_start:vae_pwm_end, :, :]\n",
    "            \n",
    "            vae_sampled_pwm_permuted = K.permute_dimensions(vae_sampled_pwm, (0, 1, 4, 2, 3))\n",
    "\n",
    "            z_param_collection = tf.map_fn(lambda x_in: K.concatenate(saved_vae_encoder_model(x_in[:, 0, ...]), axis=-1)[..., :2*vae_latent_dim], vae_sampled_pwm_permuted, parallel_iterations=16)\n",
    "\n",
    "            z_mean = K.permute_dimensions(z_param_collection[..., :vae_latent_dim], (0, 1, 2))\n",
    "            z_log_var = K.permute_dimensions(z_param_collection[..., vae_latent_dim:2*vae_latent_dim], (0, 1, 2))\n",
    "\n",
    "            z_mean = K.tile(K.expand_dims(z_mean, axis=2), (1, 1, n_z_samples, 1))\n",
    "            z_log_var = K.tile(K.expand_dims(z_log_var, axis=2), (1, 1, n_z_samples, 1))\n",
    "\n",
    "            z = get_z_samples([z_mean, z_log_var], n_z_samples=n_z_samples)\n",
    "            \n",
    "            return vae_pwm, vae_sampled_pwm, z_mean, z_log_var, z\n",
    "        \n",
    "        vae_pwm_1, vae_sampled_pwm_1, z_mean_1, z_log_var_1, z_1 = _encode_and_sample(saved_vae_encoder_model, pwm_1, sampled_pwm_1, vae_pwm_start, vae_pwm_end, vae_latent_dim, n_z_samples)\n",
    "        \n",
    "        z_1_permuted = K.permute_dimensions(z_1, (0, 2, 1, 3))\n",
    "\n",
    "        s_1 = K.zeros((batch_size, 1))\n",
    "        \n",
    "        decoded_pwm_1 = tf.map_fn(lambda z_in: tf.map_fn(lambda z_in_in: saved_vae_decoder_model([z_in_in]), z_in, parallel_iterations=16), z_1_permuted, parallel_iterations=16)\n",
    "        decoded_pwm_1 = K.expand_dims(decoded_pwm_1, axis=-3)\n",
    "        decoded_pwm_1 = K.permute_dimensions(decoded_pwm_1, (0, 2, 1, 4, 5, 3))\n",
    "\n",
    "        vae_pwm_tiled_1 = K.tile(K.expand_dims(vae_pwm_1, axis=1), (1, n_z_samples, 1, 1, 1))\n",
    "        vae_sampled_pwm_tiled_1 = K.tile(K.expand_dims(vae_sampled_pwm_1, axis=2), (1, 1, n_z_samples, 1, 1, 1))\n",
    "\n",
    "        return [vae_pwm_tiled_1, vae_sampled_pwm_tiled_1, z_mean_1, z_log_var_1, z_1, decoded_pwm_1]\n",
    "    \n",
    "    vae_elbo_layer = Lambda(_vae_elbo_func)\n",
    "    \n",
    "    #Call vae elbo estimator on generator sequences\n",
    "    vae_elbo_outputs = vae_elbo_layer([generated_sequence_pwm, generated_sequence_samples])\n",
    "    \n",
    "    return vae_elbo_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.distributions import Normal as tf_normal\n",
    "from tensorflow.contrib.distributions import percentile as tf_perc\n",
    "\n",
    "def build_loss_model_with_vae(predictor_model, loss_func, extra_loss_tensors=[]) :\n",
    "\n",
    "    loss_out = Lambda(lambda out: loss_func(out), output_shape = (1,))(predictor_model.outputs + extra_loss_tensors)\n",
    "\n",
    "    loss_model = Model(predictor_model.inputs, loss_out)\n",
    "\n",
    "    return 'loss_model', loss_model\n",
    "\n",
    "#Define target isoform loss function\n",
    "def get_earthmover_loss(target_val, variance_thresh=0.7, pwm_start=0, pwm_end=237, vae_divergence_weight=1., ref_vae_log_p=-10, vae_log_p_margin=1, decoded_pwm_epsilon=10**-6) :\n",
    "    \n",
    "    def loss_func(predictor_outputs) :\n",
    "        pwm_logits, pwm, sampled_pwm, score_pred, var_pred, _, _, vae_pwm_1, vae_sampled_pwm_1, z_mean_1, z_log_var_1, z_1, decoded_pwm_1 = predictor_outputs\n",
    "\n",
    "        #Specify costs\n",
    "        #fitness_loss = -1.0 * K.mean(score_pred[..., 0], axis=0)\n",
    "        \n",
    "        fitness_distr_w_var_grad = tf_normal(loc=score_pred[..., 0], scale=K.sqrt(var_pred[..., 0]))\n",
    "        fitness_log_sf_w_var_grad = fitness_distr_w_var_grad.log_survival_function(K.constant(target_val))\n",
    "\n",
    "        fitness_distr = tf_normal(loc=score_pred[..., 0], scale=K.stop_gradient(K.sqrt(var_pred[..., 0])))\n",
    "        fitness_log_sf = fitness_distr.log_survival_function(K.constant(target_val))\n",
    "\n",
    "        fitness_log_sf_actual = K.switch(score_pred[..., 0] < variance_thresh * K.constant(target_val), fitness_log_sf, fitness_log_sf_w_var_grad)\n",
    "\n",
    "        fitness_loss = K.mean(-fitness_log_sf_actual, axis=0)\n",
    "        \n",
    "        #Construct VAE sequence inputs\n",
    "        decoded_pwm_1 = K.clip(decoded_pwm_1, decoded_pwm_epsilon, 1. - decoded_pwm_epsilon)\n",
    "        \n",
    "        log_p_x_given_z_1 = K.sum(K.sum(vae_sampled_pwm_1 * K.log(K.stop_gradient(decoded_pwm_1)) / K.log(K.constant(10.)), axis=(-1, -2)), axis=-1)\n",
    "        \n",
    "        log_p_std_normal_1 = K.sum(normal_log_prob(z_1, 0., 1.) / K.log(K.constant(10.)), axis=-1)\n",
    "        log_p_importance_1 = K.sum(normal_log_prob(z_1, z_mean_1, K.sqrt(K.exp(z_log_var_1))) / K.log(K.constant(10.)), axis=-1)\n",
    "        \n",
    "        log_p_vae_1 = log_p_x_given_z_1 + log_p_std_normal_1 - log_p_importance_1\n",
    "        log_p_vae_div_n_1 = log_p_vae_1 - K.log(K.constant(n_z_samples, dtype='float32')) / K.log(K.constant(10.))\n",
    "\n",
    "        #Calculate mean ELBO across samples (log-sum-exp trick)\n",
    "        max_log_p_vae_1 = K.max(log_p_vae_div_n_1, axis=-1)\n",
    "\n",
    "        log_mean_p_vae_1 = max_log_p_vae_1 + K.log(K.sum(10**(log_p_vae_div_n_1 - K.expand_dims(max_log_p_vae_1, axis=-1)), axis=-1)) / K.log(K.constant(10.))\n",
    "        \n",
    "        #Specify VAE divergence loss function\n",
    "        vae_divergence_loss = vae_divergence_weight * K.mean(K.switch(log_mean_p_vae_1 < ref_vae_log_p - vae_log_p_margin, -log_mean_p_vae_1 + (ref_vae_log_p - vae_log_p_margin), K.zeros_like(log_mean_p_vae_1)), axis=0)\n",
    "        \n",
    "        #Compute total loss\n",
    "        total_loss = fitness_loss + vae_divergence_loss\n",
    "\n",
    "        return K.reshape(K.sum(total_loss, axis=0), (1,))\n",
    "    \n",
    "    def val_loss_func(predictor_outputs) :\n",
    "        pwm_logits, pwm, sampled_pwm, score_pred, var_pred, _, _ = predictor_outputs\n",
    "\n",
    "        #Specify costs\n",
    "        fitness_loss = -1.0 * K.mean(score_pred[..., 0], axis=0)\n",
    "        \n",
    "        #Compute total loss\n",
    "        total_loss = fitness_loss\n",
    "\n",
    "        return K.reshape(K.mean(total_loss, axis=0), (1,))\n",
    "    \n",
    "    return loss_func, val_loss_func\n",
    "\n",
    "\n",
    "def get_nop_transform() :\n",
    "    \n",
    "    def _transform_func(pwm) :\n",
    "        \n",
    "        return pwm\n",
    "    \n",
    "    return _transform_func\n",
    "\n",
    "class ValidationCallback(Callback):\n",
    "    def __init__(self, val_name, val_loss_model, val_steps) :\n",
    "        self.val_name = val_name\n",
    "        self.val_loss_model = val_loss_model\n",
    "        self.val_steps = val_steps\n",
    "        \n",
    "        self.val_loss_history = []\n",
    "        \n",
    "        #Track val loss\n",
    "        self.val_loss_history.append(self.val_loss_model.predict(x=None, steps=self.val_steps)[0])\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}) :\n",
    "        #Track val loss\n",
    "        val_loss_value = self.val_loss_model.predict(x=None, steps=self.val_steps)[0]\n",
    "        self.val_loss_history.append(val_loss_value)\n",
    "\n",
    "#Sequence optimization monitor during training\n",
    "class StoreSequenceMonitor(Callback):\n",
    "    def __init__(self, seqprop_model, sequence_encoder, run_dir=\"\", run_prefix=\"\", store_every1=5, store_every2=100, swap_iter=100, val_steps=1) :\n",
    "        self.seqprop_model = seqprop_model\n",
    "        self.val_steps = val_steps\n",
    "        self.sequence_encoder = sequence_encoder\n",
    "        self.run_prefix = run_prefix\n",
    "        self.run_dir = run_dir\n",
    "        self.edit_distance_samples = []\n",
    "        self.store_every1 = store_every1\n",
    "        self.store_every2 = store_every2\n",
    "        self.store_every = store_every1\n",
    "        self.swap_iter = swap_iter\n",
    "        \n",
    "        if not os.path.exists(self.run_dir): os.makedirs(self.run_dir)\n",
    "\n",
    "        seqs = self._sample_sequences()\n",
    "        #self._store_sequences(seqs, 0)\n",
    "    \n",
    "    def _sample_sequences(self) :\n",
    "        sampled_pwm = self.seqprop_model.predict(x=None, steps=self.val_steps)[2]\n",
    "        \n",
    "        seqs = []\n",
    "        for i in range(sampled_pwm.shape[1]) :\n",
    "            for j in range(sampled_pwm.shape[0]) :\n",
    "                seqs.append(self.sequence_encoder.decode(sampled_pwm[j, i, :, :, 0]))\n",
    "        \n",
    "        return seqs\n",
    "    \n",
    "    def _store_sequences(self, seqs, epoch) :\n",
    "        #Save sequences to file\n",
    "        with open(self.run_dir + self.run_prefix + \"_epoch_\" + str(epoch) + \"_\" + str(self.val_steps) + \"_steps.txt\", \"a+\") as f:\n",
    "            for i in range(len(seqs)) :\n",
    "                f.write(seqs[i] + \"\\n\")\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}) :\n",
    "        \n",
    "        if batch > self.swap_iter :\n",
    "            self.store_every = self.store_every2\n",
    "        \n",
    "        if batch % self.store_every == 0 :\n",
    "            seqs = self._sample_sequences()\n",
    "            self._store_sequences(seqs, batch)\n",
    "\n",
    "#Function for running SeqProp on a set of objectives to optimize\n",
    "def run_seqprop(run_prefix, model_path, oracle_suffix, random_state, num_models, loss_funcs, val_loss_funcs, transform_funcs, n_sequences=1, n_samples=1, n_z_samples=1, vae_params=None, n_valid_samples=1, eval_mode='sample', normalize_logits=False, n_epochs=10, steps_per_epoch=100) :\n",
    "    \n",
    "    n_objectives = 1\n",
    "    \n",
    "    seqprop_predictors = []\n",
    "    valid_monitors = []\n",
    "    train_histories = []\n",
    "    valid_histories = []\n",
    "    \n",
    "    for obj_ix in range(n_objectives) :\n",
    "        print(\"Optimizing objective \" + str(obj_ix) + '...')\n",
    "        \n",
    "        loss_func = loss_funcs[obj_ix]\n",
    "        val_loss_func = val_loss_funcs[obj_ix]\n",
    "        transform_func = transform_funcs[obj_ix]\n",
    "        \n",
    "        #Build Generator Network\n",
    "        _, seqprop_generator = build_generator(seq_length=237, n_sequences=n_sequences, n_samples=n_samples, batch_normalize_pwm=normalize_logits, pwm_transform_func=transform_func, validation_sample_mode='sample')\n",
    "        #for layer in seqprop_generator.layers :\n",
    "        #    if 'policy' not in layer.name :\n",
    "        #        layer.name += \"_trainversion\"\n",
    "        _, valid_generator = build_generator(seq_length=237, n_sequences=n_sequences, n_samples=n_valid_samples, batch_normalize_pwm=normalize_logits, pwm_transform_func=None, validation_sample_mode='sample', master_generator=seqprop_generator)\n",
    "        for layer in valid_generator.layers :\n",
    "            #if 'policy' not in layer.name :\n",
    "            layer.name += \"_valversion\"\n",
    "        \n",
    "        #Build Predictor Network and hook it on the generator PWM output tensor\n",
    "        _, seqprop_predictor = build_predictor(seqprop_generator, load_saved_predictor(model_path, oracle_suffix, random_state, num_models), n_sequences=n_sequences, n_samples=n_samples, eval_mode=eval_mode)\n",
    "        #for layer in seqprop_predictor.layers :\n",
    "        #    if '_trainversion' not in layer.name and 'policy' not in layer.name :\n",
    "        #        layer.name += \"_trainversion\"\n",
    "        _, valid_predictor = build_predictor(valid_generator, load_saved_predictor(model_path, oracle_suffix, random_state, num_models), n_sequences=n_sequences, n_samples=n_valid_samples, eval_mode='sample')\n",
    "        for layer in valid_predictor.layers :\n",
    "            if '_valversion' not in layer.name :# and 'policy' not in layer.name :\n",
    "                layer.name += \"_valversion\"\n",
    "        \n",
    "        #Build VAE model\n",
    "        vae_tensors = []\n",
    "        if vae_params is not None :\n",
    "            vae_path, vae_suffix, vae_latent_dim, vae_pwm_start, vae_pwm_end = vae_params\n",
    "            vae_tensors = build_sqp_vae(seqprop_generator, vae_path, vae_suffix, batch_size=n_sequences, seq_length=237, n_samples=n_samples, n_z_samples=n_z_samples, vae_latent_dim=vae_latent_dim, vae_pwm_start=vae_pwm_start, vae_pwm_end=vae_pwm_end)\n",
    "        \n",
    "        #Build Loss Model (In: Generator seed, Out: Loss function)\n",
    "        _, loss_model = build_loss_model_with_vae(seqprop_predictor, loss_func, extra_loss_tensors=vae_tensors)\n",
    "        _, valid_loss_model = build_loss_model(valid_predictor, val_loss_func)\n",
    "        \n",
    "        #Specify Optimizer to use\n",
    "        #opt = keras.optimizers.SGD(lr=0.5)\n",
    "        #opt = keras.optimizers.SGD(lr=0.1, momentum=0.9, decay=0, nesterov=True)\n",
    "        opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "        #Compile Loss Model (Minimize self)\n",
    "        loss_model.compile(loss=lambda true, pred: pred, optimizer=opt)\n",
    "        \n",
    "        #Specify callback entities\n",
    "        train_history = ValidationCallback('loss', loss_model, 1)\n",
    "        valid_history = ValidationCallback('val_loss', valid_loss_model, 1)\n",
    "        \n",
    "        #Standard sequence decoder\n",
    "        AA = ['a', 'r', 'n', 'd', 'c', 'q', 'e', 'g', 'h', 'i', 'l', 'k', 'm', 'f', 'p', 's', 't', 'w', 'y', 'v']\n",
    "        residue_map = {key.upper() : val for val, key in enumerate(AA)}\n",
    "        seq_encoder = IdentityEncoder(237, residue_map)\n",
    "        \n",
    "        #Build callback for printing intermediate sequences\n",
    "        store_seq_monitor = StoreSequenceMonitor(valid_generator, seq_encoder, run_dir=\"./seqprop_samples/\" + run_prefix + \"/\", run_prefix=\"intermediate\", val_steps=1)\n",
    "        \n",
    "        callbacks =[\n",
    "            train_history,\n",
    "            valid_history,\n",
    "            store_seq_monitor\n",
    "        ]\n",
    "        \n",
    "        #Fit Loss Model\n",
    "        _ = loss_model.fit(\n",
    "            [], np.ones((1, 1)), #Dummy training example\n",
    "            epochs=n_epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        train_history.val_loss_model = None\n",
    "        valid_history.val_loss_model = None\n",
    "        \n",
    "        seqprop_predictors.append(seqprop_predictor)\n",
    "        train_histories.append(train_history)\n",
    "        valid_histories.append(valid_history)\n",
    "\n",
    "    return seqprop_predictors, train_histories, valid_histories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_fitness_qt = 3.15\n"
     ]
    }
   ],
   "source": [
    "#Set target value\n",
    "\n",
    "qt = 0.95\n",
    "qt_val = 3.15\n",
    "\n",
    "print(\"target_fitness_qt = \" + str(round(qt_val, 4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_seed(seed_value) :\n",
    "    # 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "    # 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    tf.set_random_seed(seed_value)\n",
    "\n",
    "    # 5. Configure a new global `tensorflow` session\n",
    "    session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "    K.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimization experiment 'GFP'\n",
      "Optimizing objective 0...\n",
      "Epoch 1/1\n",
      "20001/20001 [==============================] - 703s 35ms/step - loss: 14.9298\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_prefix = \"seqprop_gfp_weak_balaji_20000_updates_it_\" + str(it).replace(\".\", \"_\") + \"_elbo_95th_perc\"\n",
    "\n",
    "#Run SeqProp Optimization\n",
    "\n",
    "print(\"Running optimization experiment 'GFP'\")\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "#Number of PWMs to generate per objective\n",
    "n_sequences = 10\n",
    "#Number of One-hot sequences to sample from the PWM at each grad step\n",
    "n_samples = 1\n",
    "#Number of VAE samples\n",
    "n_z_samples = 32\n",
    "#Number of epochs per objective to optimize\n",
    "n_epochs = 1\n",
    "#Number of steps (grad updates) per epoch\n",
    "steps_per_epoch = 20000 + 1\n",
    "#Either 'pwm' or 'sample'\n",
    "eval_mode = 'sample'\n",
    "#Normalize sequence logits\n",
    "normalize_logits = True\n",
    "#Number of One-hot validation sequences to sample from the PWM\n",
    "n_valid_samples = 1\n",
    "\n",
    "losses, val_losses = zip(*[\n",
    "    get_earthmover_loss(\n",
    "        target_val=qt_val,\n",
    "        pwm_start=0,\n",
    "        pwm_end=237,\n",
    "        vae_divergence_weight=10. * 1./237.,\n",
    "        ref_vae_log_p=-0.1512,\n",
    "        vae_log_p_margin=0\n",
    "    )\n",
    "])\n",
    "\n",
    "transforms = [\n",
    "    None\n",
    "]\n",
    "\n",
    "seqprop_predictors, train_histories, valid_histories = run_seqprop(run_prefix, \"models/\", oracle_suffix, it + 1, num_models, losses, val_losses, transforms, n_sequences, n_samples, n_z_samples, vae_params, n_valid_samples, eval_mode, normalize_logits, n_epochs, steps_per_epoch)\n",
    "\n",
    "seqprop_predictor, train_history, valid_history = seqprop_predictors[0], train_histories[0], valid_histories[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
