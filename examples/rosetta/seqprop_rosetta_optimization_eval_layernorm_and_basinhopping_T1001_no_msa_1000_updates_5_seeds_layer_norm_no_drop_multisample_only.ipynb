{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, ConvLSTM2D, GRU, BatchNormalization, LocallyConnected2D, Permute\n",
    "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import keras.losses\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import isolearn.keras as iso\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as spio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import isolearn.keras as iso\n",
    "\n",
    "from seqprop.visualization import *\n",
    "from seqprop_generator_protein_layer_norm import *\n",
    "from seqprop.predictor import *\n",
    "from seqprop.optimizer import *\n",
    "\n",
    "from seqprop_protein_utils import *\n",
    "from seqprop_rosetta_kl_helper import _get_kl_divergence_numpy, _get_smooth_kl_divergence_numpy, _get_smooth_circular_kl_divergence_numpy\n",
    "from seqprop_rosetta_kl_helper import _get_kl_divergence_keras, _get_smooth_kl_divergence_keras, _get_smooth_circular_kl_divergence_keras\n",
    "from basinhopping_rosetta import *\n",
    "\n",
    "from definitions.trrosetta_single_model_no_msa_batched_simpler_1d_features_2 import load_saved_predictor\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "from adam_accumulate_keras import *\n",
    "\n",
    "def contain_tf_gpu_mem_usage() :\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)\n",
    "\n",
    "contain_tf_gpu_mem_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define target isoform loss function\n",
    "def get_kl_loss(target_p_dist, target_p_theta, target_p_phi, target_p_omega) :\n",
    "    \n",
    "    def loss_func(predictor_outputs) :\n",
    "        pwm_logits, pwm, sampled_pwm, p_dist, p_theta, p_phi, p_omega = predictor_outputs\n",
    "        \n",
    "        kl_dist, kl_theta, kl_phi, kl_omega = _get_kl_divergence_keras(p_dist, p_theta, p_phi, p_omega, target_p_dist, target_p_theta, target_p_phi, target_p_omega)\n",
    "        \n",
    "        #Specify costs\n",
    "        fitness_loss = K.mean(kl_dist + kl_theta + kl_phi + kl_omega, axis=0)\n",
    "        \n",
    "        #Compute total loss\n",
    "        total_loss = fitness_loss\n",
    "\n",
    "        return K.reshape(K.sum(total_loss, axis=0), (1,))\n",
    "    \n",
    "    def val_loss_func(predictor_outputs) :\n",
    "        pwm_logits, pwm, sampled_pwm, p_dist, p_theta, p_phi, p_omega = predictor_outputs\n",
    "\n",
    "        kl_dist, kl_theta, kl_phi, kl_omega = _get_kl_divergence_keras(p_dist, p_theta, p_phi, p_omega, target_p_dist, target_p_theta, target_p_phi, target_p_omega)\n",
    "        \n",
    "        #Specify costs\n",
    "        fitness_loss = K.mean(kl_dist + kl_theta + kl_phi + kl_omega, axis=0)\n",
    "        \n",
    "        #Compute total loss\n",
    "        total_loss = fitness_loss\n",
    "\n",
    "        return K.reshape(K.sum(total_loss, axis=0), (1,))\n",
    "    \n",
    "    def val_loss_func_smooth_kl(predictor_outputs) :\n",
    "        pwm_logits, pwm, sampled_pwm, p_dist, p_theta, p_phi, p_omega = predictor_outputs\n",
    "\n",
    "        kl_dist, kl_theta, kl_phi, kl_omega = _get_smooth_kl_divergence_keras(p_dist, p_theta, p_phi, p_omega, target_p_dist, target_p_theta, target_p_phi, target_p_omega)\n",
    "        \n",
    "        #Specify costs\n",
    "        fitness_loss = K.mean(kl_dist + kl_theta + kl_phi + kl_omega, axis=0)\n",
    "        \n",
    "        #Compute total loss\n",
    "        total_loss = fitness_loss\n",
    "\n",
    "        return K.reshape(K.sum(total_loss, axis=0), (1,))\n",
    "    \n",
    "    def val_loss_func_smooth_circular_kl(predictor_outputs) :\n",
    "        pwm_logits, pwm, sampled_pwm, p_dist, p_theta, p_phi, p_omega = predictor_outputs\n",
    "\n",
    "        kl_dist, kl_theta_sin, kl_theta_cos, kl_phi, kl_omega_sin, kl_omega_cos = _get_smooth_circular_kl_divergence_keras(p_dist, p_theta, p_phi, p_omega, target_p_dist, target_p_theta, target_p_phi, target_p_omega)\n",
    "        \n",
    "        #Specify costs\n",
    "        fitness_loss = K.mean(kl_dist + kl_theta_sin + kl_theta_cos + kl_phi + kl_omega_sin + kl_omega_cos, axis=0)\n",
    "        \n",
    "        #Compute total loss\n",
    "        total_loss = fitness_loss\n",
    "\n",
    "        return K.reshape(K.sum(total_loss, axis=0), (1,))\n",
    "    \n",
    "    return loss_func, val_loss_func, val_loss_func_smooth_kl, val_loss_func_smooth_circular_kl\n",
    "\n",
    "\n",
    "def get_nop_transform() :\n",
    "    \n",
    "    def _transform_func(pwm) :\n",
    "        \n",
    "        return pwm\n",
    "    \n",
    "    return _transform_func\n",
    "\n",
    "class ValidationCallback(Callback):\n",
    "    def __init__(self, val_name, val_loss_model, val_steps) :\n",
    "        self.val_name = val_name\n",
    "        self.val_loss_model = val_loss_model\n",
    "        self.val_steps = val_steps\n",
    "        \n",
    "        self.val_loss_history = []\n",
    "        \n",
    "        #Track val loss\n",
    "        self.val_loss_history.append(self.val_loss_model.predict(x=None, steps=self.val_steps)[0])\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}) :\n",
    "        #Track val loss\n",
    "        val_loss_value = self.val_loss_model.predict(x=None, steps=self.val_steps)[0]\n",
    "        self.val_loss_history.append(val_loss_value)\n",
    "\n",
    "\n",
    "class DummyValidationCallback(Callback):\n",
    "    def __init__(self, val_name) :\n",
    "        self.val_name = val_name\n",
    "        \n",
    "        self.val_loss_history = []\n",
    "\n",
    "class DummyFlexibleSeqPropMonitor(Callback):\n",
    "    def __init__(self, measure_name='Measure') :\n",
    "        self.measure_name = measure_name\n",
    "        self.measure_history = [] = []\n",
    "        self.entropy_history = []\n",
    "        self.nt_swap_history = []\n",
    "        self.prev_optimized_pwm = None\n",
    "\n",
    "        self.n_epochs = 0\n",
    "\n",
    "def _tmp_load_model(model_path) :\n",
    "\n",
    "    saved_model = load_model(model_path, custom_objects = {\n",
    "        'InstanceNormalization' : InstanceNormalization,\n",
    "        'reweight' : reweight,\n",
    "        'wmin' : 0.8,\n",
    "        'msa2pssm' : msa2pssm,\n",
    "        'tf' : tf,\n",
    "        'fast_dca' : fast_dca,\n",
    "        'keras_collect_features' : keras_collect_features\n",
    "    })\n",
    "    \n",
    "    return saved_model\n",
    "\n",
    "\n",
    "#Function for running SeqProp on a set of objectives to optimize\n",
    "def run_seqprop(loss_funcs, val_loss_funcs, val_loss_funcs_smooth_kl, val_loss_funcs_smooth_circular_kl, transform_funcs, temperature_params, t_distos, msa_one_hots, seq_length=100, n_sequences=1, n_samples=1, n_valid_samples=1, eval_mode='sample', normalize_logits=False, n_epochs=10, steps_per_epoch=100, logit_init_mode='glorot_uniform', optimizer=None) :\n",
    "    \n",
    "    if eval_mode == 'basinhopping' :\n",
    "        residues = list(\"ARNDCQEGHILKMFPSTWYV\")\n",
    "        residue_map = {\n",
    "            residue : residue_ix\n",
    "            for residue_ix, residue in enumerate(residues)\n",
    "        }\n",
    "        acgt_encoder = IdentityEncoder(seq_length, residue_map)\n",
    "        \n",
    "        predictor = _tmp_load_model(model_path)\n",
    "        predictor.compile(\n",
    "            loss='mse',\n",
    "            optimizer=keras.optimizers.SGD(lr=0.1)\n",
    "        )\n",
    "        \n",
    "        n_iters_per_temperate, n_swaps, t_init, t_func = temperature_params[0]\n",
    "        \n",
    "        evolved_sequences, evolved_scores, evolved_scores_smooth_kl, evolved_scores_smooth_circular_kl = run_simulated_annealing_batch(predictor, t_distos, msa_one_hots[0], \"$\" * seq_length, acgt_encoder, n_sequences=n_sequences, n_iters=steps_per_epoch, n_iters_per_temperate=n_iters_per_temperate, temperature_init=t_init, temperature_func=t_func, n_swaps=n_swaps, verbose=False)\n",
    "        evolved_losses = -evolved_scores\n",
    "        evolved_losses_smooth_kl = -evolved_scores_smooth_kl\n",
    "        evolved_losses_smooth_circular_kl = -evolved_scores_smooth_circular_kl\n",
    "        \n",
    "        train_history = DummyValidationCallback('loss')\n",
    "        train_history.val_loss_history = np.mean(evolved_losses * n_sequences, axis=0).tolist()\n",
    "        \n",
    "        valid_history = DummyValidationCallback('val_loss')\n",
    "        valid_history.val_loss_history = np.mean(evolved_losses, axis=0).tolist()\n",
    "        \n",
    "        valid_history_smooth_kl = DummyValidationCallback('val_loss_smooth_kl')\n",
    "        valid_history_smooth_kl.val_loss_history = np.mean(evolved_losses_smooth_kl, axis=0).tolist()\n",
    "        \n",
    "        valid_history_smooth_circular_kl = DummyValidationCallback('val_loss_smooth_circular_kl')\n",
    "        valid_history_smooth_circular_kl.val_loss_history = np.mean(evolved_losses_smooth_circular_kl, axis=0).tolist()\n",
    "        \n",
    "        opt_pwm = np.expand_dims(np.expand_dims(acgt_encoder.encode(evolved_sequences[0]), axis=-1), axis=0)\n",
    "        \n",
    "        return [opt_pwm], [train_history], [valid_history], [valid_history_smooth_kl], [valid_history_smooth_circular_kl]\n",
    "    \n",
    "    n_objectives = len(loss_funcs)\n",
    "    \n",
    "    seqprop_predictors = []\n",
    "    valid_monitors = []\n",
    "    train_histories = []\n",
    "    valid_histories = []\n",
    "    valid_histories_smooth_kl = []\n",
    "    valid_histories_smooth_circular_kl = []\n",
    "    \n",
    "    for obj_ix in range(n_objectives) :\n",
    "        print(\"Optimizing objective \" + str(obj_ix) + '...')\n",
    "        \n",
    "        loss_func = loss_funcs[obj_ix]\n",
    "        val_loss_func = val_loss_funcs[obj_ix]\n",
    "        val_loss_func_smooth_kl = val_loss_funcs_smooth_kl[obj_ix]\n",
    "        val_loss_func_smooth_circular_kl = val_loss_funcs_smooth_circular_kl[obj_ix]\n",
    "        transform_func = transform_funcs[obj_ix]\n",
    "        msa_one_hot = msa_one_hots[obj_ix]\n",
    "        \n",
    "        #Build Generator Network\n",
    "        _, seqprop_generator = build_generator(seq_length=seq_length, n_sequences=n_sequences, n_samples=n_samples, batch_normalize_pwm=normalize_logits, pwm_transform_func=transform_func, validation_sample_mode='sample', logit_init_mode=logit_init_mode)\n",
    "        #for layer in seqprop_generator.layers :\n",
    "        #    if 'policy' not in layer.name :\n",
    "        #        layer.name += \"_trainversion\"\n",
    "        _, valid_generator = build_generator(seq_length=seq_length, n_sequences=n_sequences, n_samples=n_valid_samples, batch_normalize_pwm=normalize_logits, pwm_transform_func=None, validation_sample_mode='max', master_generator=seqprop_generator, logit_init_mode=logit_init_mode)\n",
    "        for layer in valid_generator.layers :\n",
    "            #if 'policy' not in layer.name :\n",
    "            layer.name += \"_valversion\"\n",
    "        \n",
    "        #Build Predictor Network and hook it on the generator PWM output tensor\n",
    "        _, seqprop_predictor = build_predictor(seqprop_generator, load_saved_predictor(model_path, msa_one_hot=msa_one_hot), n_sequences=n_sequences, n_samples=n_samples, eval_mode=eval_mode)\n",
    "        #for layer in seqprop_predictor.layers :\n",
    "        #    if '_trainversion' not in layer.name and 'policy' not in layer.name :\n",
    "        #        layer.name += \"_trainversion\"\n",
    "        _, valid_predictor = build_predictor(valid_generator, load_saved_predictor(model_path, msa_one_hot=msa_one_hot), n_sequences=n_sequences, n_samples=n_valid_samples, eval_mode='sample')\n",
    "        for layer in valid_predictor.layers :\n",
    "            if '_valversion' not in layer.name :# and 'policy' not in layer.name :\n",
    "                layer.name += \"_valversion\"\n",
    "        \n",
    "        #Build Loss Model (In: Generator seed, Out: Loss function)\n",
    "        _, loss_model = build_loss_model(seqprop_predictor, loss_func)\n",
    "        _, valid_loss_model = build_loss_model(valid_predictor, val_loss_func)\n",
    "        _, valid_loss_model_smooth_kl = build_loss_model(valid_predictor, val_loss_func_smooth_kl)\n",
    "        _, valid_loss_model_smooth_circular_kl = build_loss_model(valid_predictor, val_loss_func_smooth_circular_kl)\n",
    "        \n",
    "        #Compile Loss Model (Minimize self)\n",
    "        loss_model.compile(loss=lambda true, pred: pred, optimizer=optimizer)\n",
    "\n",
    "        def get_logit(p) :\n",
    "            return np.log(p / (1. - p))\n",
    "        \n",
    "        #Specify callback entities\n",
    "        #measure_func = lambda pred_outs: np.mean(get_logit(np.expand_dims(pred_outs[0], axis=0) if len(pred_outs[0].shape) <= 2 else pred_outs[0]), axis=0)\n",
    "        #measure_func = lambda pred_outs: np.mean(np.expand_dims(pred_outs[1], axis=0) if len(pred_outs[1].shape) <= 2 else pred_outs[1], axis=0)\n",
    "        \n",
    "        #train_monitor = FlexibleSeqPropMonitor(predictor=seqprop_predictor, plot_on_train_end=False, plot_every_epoch=False, track_every_step=True, measure_func=measure_func, measure_name='Binding Log Odds', plot_pwm_start=500, plot_pwm_end=700, sequence_template=sequence_template, plot_pwm_indices=np.arange(n_sequences).tolist(), figsize=(12, 1.0))\n",
    "        #valid_monitor = FlexibleSeqPropMonitor(predictor=valid_predictor, plot_on_train_end=True, plot_every_epoch=False, track_every_step=True, measure_func=measure_func, measure_name='Binding Log Odds', plot_pwm_start=500, plot_pwm_end=600, sequence_template=sequence_template, plot_pwm_indices=np.arange(n_sequences).tolist(), figsize=(12, 1.0))\n",
    "        \n",
    "        train_history = ValidationCallback('loss', loss_model, 1)\n",
    "        valid_history = ValidationCallback('val_loss', valid_loss_model, 1)\n",
    "        valid_history_smooth_kl = ValidationCallback('val_loss_smooth_kl', valid_loss_model_smooth_kl, 1)\n",
    "        valid_history_smooth_circular_kl = ValidationCallback('val_loss_smooth_circular_kl', valid_loss_model_smooth_circular_kl, 1)\n",
    "        \n",
    "        callbacks =[\n",
    "            #EarlyStopping(monitor='loss', min_delta=0.001, patience=5, verbose=0, mode='auto'),\n",
    "            #valid_monitor,\n",
    "            train_history,\n",
    "            valid_history,\n",
    "            valid_history_smooth_kl,\n",
    "            valid_history_smooth_circular_kl\n",
    "        ]\n",
    "        \n",
    "        #Fit Loss Model\n",
    "        _ = loss_model.fit(\n",
    "            [], np.ones((1, 1)), #Dummy training example\n",
    "            epochs=n_epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        #valid_monitor.predictor = None\n",
    "        train_history.val_loss_model = None\n",
    "        valid_history.val_loss_model = None\n",
    "        valid_history_smooth_kl.val_loss_model = None\n",
    "        valid_history_smooth_circular_kl.val_loss_model = None\n",
    "        \n",
    "        _, opt_pwm, _, _, _, _, _ = seqprop_predictor.predict(x=None, steps=1)\n",
    "        \n",
    "        seqprop_predictors.append(opt_pwm)\n",
    "        #valid_monitors.append(valid_monitor)\n",
    "        train_histories.append(train_history)\n",
    "        valid_histories.append(valid_history)\n",
    "        valid_histories_smooth_kl.append(valid_history_smooth_kl)\n",
    "        valid_histories_smooth_circular_kl.append(valid_history_smooth_circular_kl)\n",
    "\n",
    "    return seqprop_predictors, train_histories, valid_histories, valid_histories_smooth_kl, valid_histories_smooth_circular_kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specfiy file path to pre-trained predictor network\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), 'trRosetta/network/model2019_07')\n",
    "model_name = 'model.xaa_batched_no_drop_2.h5'\n",
    "model_path = os.path.join(save_dir, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_seed(seed_value) :\n",
    "    # 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "    # 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    tf.set_random_seed(seed_value)\n",
    "\n",
    "    # 5. Configure a new global `tensorflow` session\n",
    "    session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "    K.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 140, 21)\n",
      "(1, 140, 140, 37)\n"
     ]
    }
   ],
   "source": [
    "msa_file = \"trRosetta/example/T1001.a3m\"\n",
    "\n",
    "a3m = parse_a3m(msa_file)\n",
    "\n",
    "msa_one_hot = np.expand_dims(one_hot_encode_msa(a3m), axis=0)[:, :1, ...]\n",
    "\n",
    "print(msa_one_hot.shape)\n",
    "\n",
    "npz_file = \"trRosetta/example/T1001_keras_xaa_corrected.npz\"\n",
    "\n",
    "npz_data = np.load(npz_file)\n",
    "\n",
    "t_pd, t_pt, t_pp, t_po = npz_data['dist'], npz_data['theta'], npz_data['phi'], npz_data['omega']\n",
    "\n",
    "print(t_pd.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "seeds = [1177, 14755, 74662, 112110, 252654]\n",
    "\n",
    "#Run SeqProp Optimization\n",
    "\n",
    "print(\"Running optimization experiment 'trRosetta Target Structure'\")\n",
    "\n",
    "#Number of PWMs to generate per objective\n",
    "n_sequences = 1\n",
    "#Number of epochs per objective to optimize\n",
    "n_epochs = 1\n",
    "#Sequence length\n",
    "seq_length = msa_one_hot.shape[2]\n",
    "#Number of steps (grad updates) per epoch\n",
    "steps_per_epoch = 1000 * 2\n",
    "#Number of One-hot validation sequences to sample from the PWM\n",
    "n_valid_samples = 1\n",
    "\n",
    "experiment_name_list = [\n",
    "    'Sampled-IN 10x (Adam 0.01 0.5/0.9)'\n",
    "]\n",
    "\n",
    "eval_mode_list = [\n",
    "    'sample'\n",
    "]\n",
    "\n",
    "normalize_logits_list = [\n",
    "    True\n",
    "]\n",
    "\n",
    "n_samples_list = [\n",
    "    5\n",
    "]\n",
    "\n",
    "temperature_params_list = [\n",
    "    [None, None, None, None]\n",
    "]\n",
    "\n",
    "logit_init_modes = [\n",
    "    'glorot_uniform'\n",
    "]\n",
    "\n",
    "optimizers = [\n",
    "    lambda x: AdamAccumulate(lr=0.01, beta_1=0.5, beta_2=0.9, accum_iters=2)\n",
    "]\n",
    "\n",
    "result_dict = {\n",
    "    'Sampled-IN 10x (Adam 0.01 0.5/0.9)' : {}\n",
    "}\n",
    "\n",
    "for experiment_name, eval_mode, normalize_logits, n_samples, temperature_params, logit_init_mode, optimizer_func in zip(experiment_name_list, eval_mode_list, normalize_logits_list, n_samples_list, temperature_params_list, logit_init_modes, optimizers) :\n",
    "    \n",
    "    print(\"Experiment name = \" + str(experiment_name))\n",
    "    print(\"Eval mode = \" + str(eval_mode))\n",
    "    print(\"Normalize logits = \" + str(normalize_logits))\n",
    "\n",
    "    for rand_seed_ix, rand_seed in enumerate(seeds) :\n",
    "    \n",
    "        print(\"Running seed \" + str(rand_seed_ix) + \"...\")\n",
    "\n",
    "        K.clear_session()\n",
    "        \n",
    "        optimizer = optimizer_func(None)\n",
    "\n",
    "        set_seed(rand_seed)\n",
    "\n",
    "        losses, val_losses, val_losses_smooth_kl, val_losses_smooth_circular_kl = zip(*[\n",
    "            get_kl_loss(\n",
    "                t_pd,\n",
    "                t_pt,\n",
    "                t_pp,\n",
    "                t_po\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        transforms = [\n",
    "            None\n",
    "        ]\n",
    "\n",
    "        opt_pwms, train_histories, valid_histories, valid_histories_smooth_kl, valid_histories_smooth_circular_kl = run_seqprop(losses, val_losses, val_losses_smooth_kl, val_losses_smooth_circular_kl, transforms, [temperature_params], [t_pd, t_pt, t_pp, t_po], [msa_one_hot], seq_length, n_sequences, n_samples, n_valid_samples, eval_mode, normalize_logits, n_epochs, steps_per_epoch, logit_init_mode, optimizer)\n",
    "\n",
    "        opt_pwm, train_history, valid_history, valid_history_smooth_kl, valid_history_smooth_circular_kl = opt_pwms[0], train_histories[0], valid_histories[0], valid_histories_smooth_kl[0], valid_histories_smooth_circular_kl[0]\n",
    "\n",
    "        if rand_seed_ix > 0 :\n",
    "            result_dict[experiment_name]['train_history'].val_loss_history.append(train_history.val_loss_history)\n",
    "            result_dict[experiment_name]['valid_history'].val_loss_history.append(valid_history.val_loss_history)\n",
    "            result_dict[experiment_name]['valid_history_smooth_kl'].val_loss_history.append(valid_history_smooth_kl.val_loss_history)\n",
    "            result_dict[experiment_name]['valid_history_smooth_circular_kl'].val_loss_history.append(valid_history_smooth_circular_kl.val_loss_history)\n",
    "            \n",
    "        else :\n",
    "            train_history.val_loss_history = [train_history.val_loss_history]\n",
    "            valid_history.val_loss_history = [valid_history.val_loss_history]\n",
    "            valid_history_smooth_kl.val_loss_history = [valid_history_smooth_kl.val_loss_history]\n",
    "            valid_history_smooth_circular_kl.val_loss_history = [valid_history_smooth_circular_kl.val_loss_history]\n",
    "            \n",
    "            result_dict[experiment_name] = {\n",
    "                'opt_pwm' : opt_pwm,\n",
    "                'train_history' : train_history,\n",
    "                'valid_history' : valid_history,\n",
    "                'valid_history_smooth_kl' : valid_history_smooth_kl,\n",
    "                'valid_history_smooth_circular_kl' : valid_history_smooth_circular_kl\n",
    "            }\n",
    "    \n",
    "    result_dict[experiment_name]['train_history'].val_loss_history_median = np.median(np.array(result_dict[experiment_name]['train_history'].val_loss_history), axis=0).tolist()\n",
    "    result_dict[experiment_name]['valid_history'].val_loss_history_median = np.median(np.array(result_dict[experiment_name]['valid_history'].val_loss_history), axis=0).tolist()\n",
    "    result_dict[experiment_name]['valid_history_smooth_kl'].val_loss_history_median = np.median(np.array(result_dict[experiment_name]['valid_history_smooth_kl'].val_loss_history), axis=0).tolist()\n",
    "    result_dict[experiment_name]['valid_history_smooth_circular_kl'].val_loss_history_median = np.median(np.array(result_dict[experiment_name]['valid_history_smooth_circular_kl'].val_loss_history), axis=0).tolist()\n",
    "    \n",
    "    result_dict[experiment_name]['train_history'].val_loss_history_mean = np.mean(np.array(result_dict[experiment_name]['train_history'].val_loss_history), axis=0).tolist()\n",
    "    result_dict[experiment_name]['valid_history'].val_loss_history_mean = np.mean(np.array(result_dict[experiment_name]['valid_history'].val_loss_history), axis=0).tolist()\n",
    "    result_dict[experiment_name]['valid_history_smooth_kl'].val_loss_history_mean = np.mean(np.array(result_dict[experiment_name]['valid_history_smooth_kl'].val_loss_history), axis=0).tolist()\n",
    "    result_dict[experiment_name]['valid_history_smooth_circular_kl'].val_loss_history_mean = np.mean(np.array(result_dict[experiment_name]['valid_history_smooth_circular_kl'].val_loss_history), axis=0).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for experiment_name in result_dict :\n",
    "    for loss_str in result_dict[experiment_name] :\n",
    "        if 'model' in dir(result_dict[experiment_name][loss_str]) :\n",
    "            result_dict[experiment_name][loss_str].model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump({'result_dict' : result_dict}, open(\"eval_seqprop_rosetta_T1001_no_msa_and_basinhopping_kl_experiment_1000_updates_5_seeds_with_multisample_layer_norm_no_drop_results_part_2.pickle\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_dict_part_2 = pickle.load(open(\"eval_seqprop_rosetta_T1001_no_msa_and_basinhopping_kl_experiment_1000_updates_5_seeds_with_multisample_layer_norm_no_drop_results_part_2.pickle\", \"rb\"))['result_dict']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for experiment_name in result_dict :\n",
    "    for loss_str in result_dict[experiment_name] :\n",
    "        if 'model' in dir(result_dict[experiment_name][loss_str]) :\n",
    "            result_dict_part_2[experiment_name][loss_str].val_loss_history_median = [result_dict_part_2[experiment_name][loss_str].val_loss_history_median[i+1] for i in range(-1, n_epochs * steps_per_epoch) if (i+1) % 2 == 0]\n",
    "            result_dict_part_2[experiment_name][loss_str].val_loss_history_mean = [result_dict_part_2[experiment_name][loss_str].val_loss_history_mean[i+1] for i in range(-1, n_epochs * steps_per_epoch) if (i+1) % 2 == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump({'result_dict' : result_dict_part_2}, open(\"eval_seqprop_rosetta_T1001_no_msa_and_basinhopping_kl_experiment_1000_updates_5_seeds_with_multisample_layer_norm_no_drop_results_part_2_collapsed_steps.pickle\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
